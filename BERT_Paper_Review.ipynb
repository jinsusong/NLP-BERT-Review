{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_paper.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyME+R/Fs8fbEIvBdynHdzVJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinsusong/NLP-BERT-Review/blob/main/BERT_Paper_Review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT 논문 리뷰\n",
        "\n",
        "Pretrained LM 표현을 downstream task에 적용하는 방법은 2가지\n",
        "\n",
        "• Feature based\n",
        "\n",
        "   • ELMo\n",
        "\n",
        "• Fine-tuning\n",
        "\n",
        "   • GPT-1\n",
        "\n",
        "• 2개 다 unidirectional language model 구조\n",
        "\n",
        "과거 형태의 문제점\n",
        "\n",
        "• Pre-trained의 힘을 제한하는 형태\n",
        "\n",
        "   • Unidirectional 구조를 가지고 있음\n",
        "\n",
        "• GPT-1을 예로 들자면 Left-to-right 아키텍처를 가지고 있음\n",
        "\n",
        "   • 모든 input token이 이전 token에만 attend할 수 있음\n",
        "\n",
        "• 이러한 구조는 token-level task의 fine-tuning시 해로울 수 있음\n",
        "\n",
        "\n",
        "• Fine-tuning based approaches인 BERT를 소개\n",
        "\n",
        "   • 기존 unidirectionality 제약을 완화\n",
        "\n",
        "      • MLM(Masked Language Modeling)을 사용해서\n",
        "\n",
        "      • 랜덤하게 마스킹 처리하고 해당 원본 단어를 예측\n",
        "\n",
        "     • 이 때문에 left, right를 전부 융합해서 봐야해서 deep bidirectional 구조가 됌\n",
        "\n",
        "   • Next Sentence Prediction(NSP) task까지 있음\n",
        "\n",
        "• 따라서 해당 BERT 논문에서 말하고자 하는 바는\n",
        "\n",
        "   • Importance of bidirectional pre-trianing LM의 중요성을 나타내고자 함\n",
        "\n",
        "   • MLM구조의 장점 발동\n",
        "\n",
        "      • Unidirectional LM (GPT-1)과 Shallow concatenation 구조인 ELMO와 반대"
      ],
      "metadata": {
        "id": "LGGZhl90UGOw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Pre-training\n",
        "\n",
        "   • Masked LM\n",
        "\n",
        "      • Deep bidirectional (LTR 모델보다 더 강력)\n",
        "\n",
        "      • 전통적인 LM은 LTR, RTL 방법\n",
        "\n",
        "   • Input token을 random하게  mask\n",
        "\n",
        "      • 15%\n",
        "\n",
        "   • Denoising Auto Encoder와 다른 점\n",
        "\n",
        "      • 전체 구성을 재구성하는게 아닌, Masked words만 예측\n",
        "\n",
        "   • Pre-training과 fine-tuning mismatch 존재\n",
        "\n",
        "      • [MASK]가 fine-tuning 과정에서 나오지 않기 때문\n",
        "\n",
        "      • 이를 완화하기 위한 조취로 MASK로 선정된 것이 항상 [MASK]되지 않음\n",
        "\n",
        "      • [MASK]된 것 중 80%는 [MASK]로 되고 10%로는 다른 token, 10% 바꾸지 않음\n",
        "\n",
        "   • Next Sentence Prediction(NSP)\n",
        "\n",
        "      • 50% 확률로 실제 다음 문장을 추출하고 50%는 아닌 것을 추출\n",
        "\n",
        "      • MSM과 함께 해당 문장이 다음 문장인지 같이 학습\n",
        "\n",
        "      • QA, NLI task에서 도움이 되었다고 함\n",
        "\n",
        "• Fine-tuning\n",
        "\n",
        "   • Task specific inputs과 outputs을 BERT에 넣어주고 end-to-end로 fine-tune\n",
        "\n",
        "   • Fine-tuning마다 input 형태가 다를 수 있음\n"
      ],
      "metadata": {
        "id": "uVyZ-wceQsRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PR2CoZK0T_t8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}