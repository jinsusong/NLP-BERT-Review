{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT Pretraining 방법 .ipynb",
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOJ0Q8tDO7hawq8P1q+GiHY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinsusong/NLP-BERT-Review/blob/main/BERT_Pretraining_%EB%B0%A9%EB%B2%95_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Instance로 Pretraining 시키는 과정"
      ],
      "metadata": {
        "id": "Sf-GE5WjTCI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Training Instance를 만드는 작업이 마무리되면 "
      ],
      "metadata": {
        "id": "g5pWorKpTaqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "<입력 Sequence - 인코딩 됨>\n",
        "\n",
        " 2      4    523  8 312   1   53 5234   4   323  123   3    21  33  22   21  123   3\n",
        "\n",
        "<입력 Mask> (Padding을 마스킹하기 위해서)\n",
        "\n",
        "1 ... 1 0 ... 0 (뒤에 0들은 padding 된 index들)\n",
        "\n",
        "\n",
        "<마스킹 된 토큰의 라벨 - 인코딩 됨>\n",
        "\n",
        "3444, 553\n",
        "\n",
        "\n",
        "<마스킹 된 토큰의 위치>\n",
        "\n",
        "1, 8\n",
        "\n",
        "\n",
        "<Random Nex>\n",
        "\n",
        "1 (True)\n",
        "\n",
        "\n",
        "<Sequence ID> (해당 토큰이 Seq_A에 속하는지 Seq_B에 속하는지)\n",
        "\n",
        "0 ... 0 1 ... 1\n",
        " \n",
        "\n",
        "이제 Training Instance를 사용해서 BERT를 Pretraining 시킬 차례"
      ],
      "metadata": {
        "id": "6UzOl-qZTlie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding"
      ],
      "metadata": {
        "id": "AxCjvxe9Tk1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT를 Pretraining 시키기 전에 Encoding 한 토큰을 Embedding 해야 함."
      ],
      "metadata": {
        "id": "8wVPlNWfT7ms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Encoding을 통해 토큰을 구분되는 Index로 변화시킴\n",
        "\n",
        "- 하지만 각 단어가 고유한 Index를 가지기 때문에 (One-hot Vector과 비슷하게), 단어 사이에 의미의 유사성이 없음.\n",
        "\n",
        "- 단어를 Encoding만 하게 되면 모델로써는 두 단어가 어떤 관계에 있는지 알 수 없음.\n",
        "\n",
        "- 이 문제를 해결하기 위해 나온 방법이 Embedding이다. \n",
        "\n",
        "- 이외에도 Embedding을 하는 이유는 굉장히 많다.\n",
        "\n"
      ],
      "metadata": {
        "id": "lMJl4dshU2UC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BERT 는 세 종류의 Embedding을 거치는데 각각 아래와 같다. \n",
        "\n",
        "- 이해를 돕기 위해 Batch Size = 8 , Sequence Length = 128 인 상황을 가정\n",
        "\n",
        "- Sequence Length :  [CLS] Seq_A [SEP] Seq_B [SEP]에 있는 토큰들의 개수\n",
        "\n",
        "- hidden_layer_size = embedding size = 768 : 한 토큰을 Embedding 할 때 길이가 768인 벡터로 Embedding 하겠다는 뜻"
      ],
      "metadata": {
        "id": "ySZjjvXnVsRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Embedding  : Word Index Scalar > Embedding Vector\n",
        "\n",
        "- torch.Size([8, 128]) > torch.Size([8, 128, 768])  / Layer Size : (32200, 768)\n",
        "\n",
        "- 토큰의 Index (BERT의 경우 0 이상 32199 이하의 정수)가 길이가 768인 Vector로 바뀌는 Embedding 이다. \n",
        "\n",
        "- 따라서 스칼라가 벡터로 변환되었다. 물론 좀 더 자세히 보면, 스칼라가 One-hot Vecotr로 바뀌고, 그다음에 길이 768인 Vector로 전환되겠지만 말이다.\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "okFbSprqWGE8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Position Embedding  : Position Index Scalar > Embedding Vector\n",
        "- torch.Size([8, 128]) > torch.Size([8, 128, 768]) / Layer Size : (128, 768)\n",
        "\n",
        " \n",
        "\n",
        "- BERT의 Input에는 해당 토큰의 위치가 어디인지에 대한 정보가 없다. 하지만 사람이 문장을 해석할 때 단어 위치는 굉장히 중요한 지표이므로, 이 정보를 추가해주어야 한다. 위 예시에서 Position Index Scalara 은 0~127 사이의 스칼라 값을 가질 것이다."
      ],
      "metadata": {
        "id": "w5U2_d7uYdd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Token Type Embedding  : Token Type Scalar > Embedding Vector\n",
        "\n",
        "- torch.Size([8, 128]) > torch.Size([8, 128, 768]) / Layer Size : (2, 768)\n",
        "\n",
        "- 토큰 타입이랑 해당 토큰이 Seq_A 에 속하는지, Seq_B에 속하는지 에 대한 정보이다. A에 속하면 0, B에 속하면 1의 값을 가질 것이다. 따라서 이 또한 스칼라에서 벡터로의 Embedding이다. \n",
        "\n",
        "- 중요한 것은 세 Embedding 모두 Layer이라는 것이다. \n",
        "\n",
        "- 즉, Model이 학습할 때 그 대상이라는 것이다. PyTorch에서는 torch.nn.Embedding 모듈을 사용하며 아래 코드에서 확인할 수 있다. \n",
        "\n",
        "- torch.nn.Embedding의 모델 구조는 FFNN (Fully Connected)"
      ],
      "metadata": {
        "id": "dCPBAX7SYjd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super(BertEmbeddings, self).__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)  # (32200, 768)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)  # (128, 768)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)  # (2, 768)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)  # 0.1\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids):\n",
        "        seq_length = input_ids.size(1)  # 128\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device) # torch.Size([128])\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids) # torch.Size([8, 128])\n",
        "\n",
        "        words_embeddings = self.word_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings  # (batchSize, sequenceLength, hidden_size)"
      ],
      "metadata": {
        "id": "dfMq2I21hfEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 요약하면 \n",
        "# Embedding Vector = Dropout(Normalize(Word Embedding + Position Embedding + Token Type Embedding))\n",
        " "
      ],
      "metadata": {
        "id": "zA0-d4B_Vpcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT Layer"
      ],
      "metadata": {
        "id": "6A3N4uPqiW2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - BERT는 Transformer의 Encoder 를 직렬로 이은 구조를 갖는다.\n",
        " \n",
        " - BERT Block (Encoder Block), 혹은 BERT Layer 이라고 함. \n",
        " \n",
        " - 한 Bert Layer은 세 가지 Layer로 구성되어 있다.\n",
        "    -  BERT Attention\n",
        "        1. Bert Self Attention (hidden_state, attention_mask)\n",
        "        2. Bert Self Output (context, hidden_state)\n",
        "    \n",
        "    - BERT Intermediate\n",
        "\n",
        "    - BERT Output\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "5V-wvnDsiarI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " BERT Attention"
      ],
      "metadata": {
        "id": "xn-O9qmViw_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bert Self Attention (hidden_state, attention_mask)\n",
        "\n",
        "# 각각의 토큰에 대해서 Query, Key, Value를 만들고 Self Attention을 수행\n",
        "\n",
        "# Multi Head Attentoin이기 때문에 768길이 Query, Key, Value를 12개의 64 길이로 나누는 것\n",
        "\n",
        " class BertSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(BertSelfAttention, self).__init__()\n",
        "        # hidden_size = 768, num_attention_heads = 12\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
        "        self.num_attention_heads = config.num_attention_heads  # 12\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)  # 64\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size  # 768\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)  # (768, 768)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)  # (768, 768)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)  # (768, 768)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)  # 0.1\n",
        "\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        #  (batch_size, seq_length, num_attention_heads, attention_head_size)\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        # Multi-Head Self Attention\n",
        "        # (batch_size, seq_length, all_head_size) -> (batch_size, seq_length, num_attention_heads, attention_head_size)\n",
        "        x = torch.reshape(x, new_x_shape)\n",
        "        \n",
        "        return x.permute(0, 2, 1, 3)  # (batch_size, 12, 128, 64)\n",
        "\n",
        "\n",
        "    def transpose_key_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = torch.reshape(x, new_x_shape)\n",
        "        \n",
        "        return x.permute(0, 2, 3, 1)   # (batch_size, 12, 64, 128)\n",
        "\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        mixed_query_layer = self.query(hidden_states)  # (batchSize, 128, 768)\n",
        "        mixed_key_layer = self.key(hidden_states)  # (batchSize, 128, 768)\n",
        "        mixed_value_layer = self.value(hidden_states)  # (batchSize, 128, 768)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)  # (batch_size, 12, 128, 64)\n",
        "        key_layer = self.transpose_key_for_scores(mixed_key_layer)  # (batch_size, 12, 64, 128)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)  # (batch_size, 12, 128, 64)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer)  # (batch_size, 12, 128, 128)\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        attention_scores = attention_scores + attention_mask \n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)  # (batch_size, 12, 128, 64)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()  # (batch_size, 128, 12, 64)\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = torch.reshape(context_layer, new_context_layer_shape)  # (batch_size, 128, 768)\n",
        "        \n",
        "        return context_layer"
      ],
      "metadata": {
        "id": "chyNqK8iizU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bert Self Output (context, hidden_state)\n",
        "\n",
        "# Self Attention Layer을 거쳐 나온 Vecotr에 FFNN을 한 번 거치고, Skip Connection을 적용\n",
        "\n",
        "class BertSelfOutput(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(BertSelfOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)  # (768, 768)\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)  # (batch_size, 128, 768)\n",
        "        hidden_states = self.dropout(hidden_states)  # (batch_size, 128, 768)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor) # Skip Connection\n",
        "\t\t\n",
        "        return hidden_states\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "N4aejl46i77I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT Intermediate \n",
        "\n",
        "# Embedding Size 가 768이었던 Vecotr을 3072 (4배)로 늘려주는 FFNN을 거친다. Activation Function으로는 gelu를 사용\n",
        "\n",
        "class BertIntermediate(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(BertIntermediate, self).__init__()\n",
        "        # (8, 128, 768) -> (8, 128, 3072)\n",
        "        self.dense_act = LinearActivation(config.hidden_size, config.intermediate_size, act=config.hidden_act)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense_act(hidden_states)\n",
        "        \n",
        "        return hidden_states\n"
      ],
      "metadata": {
        "id": "yiP-fG3sjtxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT Output\n",
        "\n",
        "# 이전에 3072로 늘어났던 Size를 다시 768로 바꾸고, Dropout & Normalize를 적용해준다. 여기서 나온 Output이 다음 Bert Layer의 Input으로 들어가게 된다. (다음 블록의 BERT Self Attention)\n",
        "\n",
        "class BertOutput(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(BertOutput, self).__init__()\n",
        "        # (8, 128, 3072) -> (8, 128, 768)\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        \n",
        "        return hidden_states  # (8, 128, 768)\n"
      ],
      "metadata": {
        "id": "9Ralhd3Hj1Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT Pooler\n"
      ],
      "metadata": {
        "id": "1SbnZQPIkJYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BERT 모델의 최종 Output으로 2가지를 내놓는다.\n",
        "\n",
        "    1. tanh(마지막 hidden_state의 첫 토큰) -> -1~1 사이의 Scala 값 / Result : (8, 2)\n",
        "        - 1번으로는 A와 B가 연속된 Sequence 인지 (Random Next) 판단하고,\n",
        "\n",
        "    2. 마지막 hidden_state / Result : (8, 128, 32200)\n",
        "        - 2번으로는 마스킹된 단어를 예측하게 된다.\n",
        "        - 2번에서 길이 768 Vector을 32200 Vector로 바꾸게 되는데, 이때 layer으로는 Word Embedding Layer을 쓴다.\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        " \n",
        "\n"
      ],
      "metadata": {
        "id": "p0q0qwZ2kM8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jkf1MUFoksh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "YrgzjFAkNeoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "from torch.nn import functional as F\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased',    return_dict = True)\n",
        "\n",
        "text = \"The capital of France, \" + tokenizer.mask_token + \", contains the Eiffel Tower.\"\n",
        "input = tokenizer.encode_plus(text, return_tensors = \"pt\")\n",
        "mask_index = torch.where(input[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
        "output = model(**input)\n",
        "logits = output.logits\n",
        "softmax = F.softmax(logits, dim = -1)\n",
        "mask_word = softmax[0, mask_index, :]\n",
        "top_10 = torch.topk(mask_word, 10, dim = 1)[1][0]\n",
        "for token in top_10:\n",
        "   word = tokenizer.decode([token])\n",
        "   new_sentence = text.replace(tokenizer.mask_token, word)\n",
        "   print(new_sentence)"
      ],
      "metadata": {
        "id": "B7M5D28sNeu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "l4F-4gYkNe-1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}