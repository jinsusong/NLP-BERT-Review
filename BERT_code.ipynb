{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_code.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPeRZYezn9OLgyEbnZ2vK/C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinsusong/NLP-BERT-Review/blob/main/BERT_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0d34itnED4W"
      },
      "outputs": [],
      "source": [
        "\"\"\"The main BERT model and related functions.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import copy\n",
        "import json\n",
        "import math\n",
        "import re\n",
        "import numpy as np\n",
        "import six\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BertConfig(object):\n",
        "  \"\"\"Configuration for `BertModel`.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               vocab_size,\n",
        "               hidden_size=768,\n",
        "               num_hidden_layers=12,\n",
        "               num_attention_heads=12,\n",
        "               intermediate_size=3072,\n",
        "               hidden_act=\"gelu\",\n",
        "               hidden_dropout_prob=0.1,\n",
        "               attention_probs_dropout_prob=0.1,\n",
        "               max_position_embeddings=512,\n",
        "               type_vocab_size=16,\n",
        "               initializer_range=0.02):\n",
        "    \"\"\"Constructs BertConfig.\n",
        "    Args:\n",
        "      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n",
        "      hidden_size: Size of the encoder layers and the pooler layer.\n",
        "      num_hidden_layers: Number of hidden layers in the Transformer encoder.\n",
        "      num_attention_heads: Number of attention heads for each attention layer in the Transformer encoder.\n",
        "      intermediate_size: The size of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n",
        "      hidden_act: The non-linear activation function (function or string) in the encoder and pooler.\n",
        "      hidden_dropout_prob: The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n",
        "      attention_probs_dropout_prob: The dropout ratio for the attention probabilities.\n",
        "      max_position_embeddings: The maximum sequence length that this model might ever be used with. Typically set this to something large just in case(e.g., 512 or 1024 or 2048).\n",
        "      type_vocab_size: The vocabulary size of the `token_type_ids` passed into `BertModel`.\n",
        "      initializer_range: The stdev of the truncated_normal_initializer for initializing all weight matrices.\n",
        "\n",
        "        vocab_size: BertModel'에서 'inputs_ids'의 어휘 크기입니다.\n",
        "        hidden_size: 인코더 계층 및 풀러 계층의 크기입니다.\n",
        "        num_hidden_discutes: 트랜스포머 인코더의 숨겨진 레이어 수입니다.\n",
        "        num_attention_heads: 트랜스포머 인코더의 각 주의 계층에 대한 주의 헤드 수입니다.\n",
        "        intermediate_size: 트랜스포머 인코더의 \"중간\"(즉, 피드 포워드) 층의 크기입니다.\n",
        "        hidden_act: 인코더 및 풀러의 비선형 활성화 기능(기능 또는 문자열)입니다.\n",
        "        hidden_dropout_prob: 임베딩, 인코더 및 풀러에서 완전히 연결된 모든 계층에 대한 드롭아웃 확률입니다.\n",
        "        attention_probs_dropout_prob: 주의 확률에 대한 드롭아웃 비율입니다.\n",
        "        max_position_clinding: 이 모델에 사용될 수 있는 최대 시퀀스 길이입니다. 일반적으로 512, 1024 또는 2048과 같은 경우에 대비하여 이 값을 큰 값으로 설정합니다.\n",
        "        type_vocab_size: 'token_type_ids'의 어휘 크기가 'BertModel'로 전달되었습니다.\n",
        "        initializer_range: 모든 가중치 행렬을 초기화하기 위한 잘린_정규_초기화기의 stdev입니다.\n",
        "    \"\"\"\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_hidden_layers = num_hidden_layers\n",
        "    self.num_attention_heads = num_attention_heads\n",
        "    self.hidden_act = hidden_act\n",
        "    self.intermediate_size = intermediate_size\n",
        "    self.hidden_dropout_prob = hidden_dropout_prob\n",
        "    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "    self.max_position_embeddings = max_position_embeddings\n",
        "    self.type_vocab_size = type_vocab_size\n",
        "    self.initializer_range = initializer_range\n",
        "\n",
        "  @classmethod\n",
        "  def from_dict(cls, json_object):\n",
        "    \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters. Python 매개 변수 사전에서 'BertConfig'를 구성합니다. \"\"\"\n",
        "    config = BertConfig(vocab_size=None)\n",
        "    for (key, value) in six.iteritems(json_object):\n",
        "      config.__dict__[key] = value\n",
        "    return config\n",
        "\n",
        "  @classmethod\n",
        "  def from_json_file(cls, json_file):\n",
        "    \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n",
        "    with tf.gfile.GFile(json_file, \"r\") as reader:\n",
        "      text = reader.read()\n",
        "    return cls.from_dict(json.loads(text))\n",
        "\n",
        "  def to_dict(self):\n",
        "    \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "    output = copy.deepcopy(self.__dict__)\n",
        "    return output\n",
        "\n",
        "  def to_json_string(self):\n",
        "    \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
      ],
      "metadata": {
        "id": "UXCXx05rEIaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertModel(object):\n",
        "  \"\"\"BERT model (\"Bidirectional Encoder Representations from Transformers\").\n",
        "  Example usage:\n",
        "  ```python\n",
        "  # Already been converted into WordPiece token ids\n",
        "  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n",
        "  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n",
        "  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n",
        "  config = modeling.BertConfig(vocab_size=32000, hidden_size=512, num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "  model = modeling.BertModel(config=config, is_training=True, input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n",
        "  label_embeddings = tf.get_variable(...)\n",
        "  pooled_output = model.get_pooled_output()\n",
        "  logits = tf.matmul(pooled_output, label_embeddings)\n",
        "  ...\n",
        "  ```\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               config,\n",
        "               is_training,\n",
        "               input_ids,\n",
        "               input_mask=None,\n",
        "               token_type_ids=None,\n",
        "               use_one_hot_embeddings=False,\n",
        "               scope=None):\n",
        "    \"\"\"Constructor for BertModel.\n",
        "    Args:\n",
        "      config: `BertConfig` instance.\n",
        "      is_training: bool. true for training model, false for eval model. Controls whether dropout will be applied.\n",
        "      input_ids: int32 Tensor of shape [batch_size, seq_length].\n",
        "      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
        "      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
        "      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word embeddings or tf.embedding_lookup() for the word embeddings.\n",
        "      scope: (optional) variable scope. Defaults to \"bert\".\n",
        "\n",
        "        config: 'BertConfig' 인스턴스입니다.\n",
        "        is_training: bool. 교육 모형에 대해서는 참이고 평가 모형에 대해서는 거짓입니다. 드롭아웃 적용 여부를 제어합니다.\n",
        "        input_ids: int32 형상의 텐서 [batch_size, seq_length].\n",
        "        input_mask: (옵션) int32 형상의 텐서 [batch_size, seq_length].\n",
        "        token_type_ids: (옵션) 셰이프의 int32 텐서 [batch_size, seq_length].\n",
        "        use_one_hot_committing: (옵션) bool. 단어 임베딩에 원핫 단어 임베딩을 사용할지 아니면 tf.embedding_lookup()을 사용할지 여부입니다.\n",
        "        scope: (선택 사항) 가변 범위. 기본값은 \"bert\"입니다.\n",
        "    Raises:\n",
        "      ValueError: The config is invalid or one of the input tensor shapes is invalid.\n",
        "    \"\"\"\n",
        "    config = copy.deepcopy(config)\n",
        "    if not is_training:\n",
        "      config.hidden_dropout_prob = 0.0\n",
        "      config.attention_probs_dropout_prob = 0.0\n",
        "\n",
        "    input_shape = get_shape_list(input_ids, expected_rank=2)\n",
        "    batch_size = input_shape[0]\n",
        "    seq_length = input_shape[1]\n",
        "\n",
        "    if input_mask is None:\n",
        "      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n",
        "\n",
        "    if token_type_ids is None:\n",
        "      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n",
        "\n",
        "    with tf.variable_scope(scope, default_name=\"bert\"):\n",
        "      with tf.variable_scope(\"embeddings\"):\n",
        "        # Perform embedding lookup on the word ids. 단어 ID에 대한 임베딩 조회를 수행합니다.\n",
        "        (self.embedding_output, self.embedding_table) = embedding_lookup(\n",
        "            input_ids=input_ids,\n",
        "            vocab_size=config.vocab_size,\n",
        "            embedding_size=config.hidden_size,\n",
        "            initializer_range=config.initializer_range,\n",
        "            word_embedding_name=\"word_embeddings\",\n",
        "            use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "        # Add positional embeddings and token type embeddings, then layer 위치 임베딩 및 토큰 유형 임베딩을 추가한 다음 레이어\n",
        "        # normalize and perform dropout. 정규화하고 중퇴를 수행합니다.\n",
        "        self.embedding_output = embedding_postprocessor(\n",
        "            input_tensor=self.embedding_output,\n",
        "            use_token_type=True,\n",
        "            token_type_ids=token_type_ids,\n",
        "            token_type_vocab_size=config.type_vocab_size,\n",
        "            token_type_embedding_name=\"token_type_embeddings\",\n",
        "            use_position_embeddings=True,\n",
        "            position_embedding_name=\"position_embeddings\",\n",
        "            initializer_range=config.initializer_range,\n",
        "            max_position_embeddings=config.max_position_embeddings,\n",
        "            dropout_prob=config.hidden_dropout_prob)\n",
        "\n",
        "      with tf.variable_scope(\"encoder\"):\n",
        "        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D mask of shape [batch_size, seq_length, seq_length] which is used for the attention scores.\n",
        "        # 이것은 모양[batch_size, seq_length]의 2D 마스크를 주의 점수에 사용되는 모양[batch_size, seq_length, seq_length]의 3D 마스크로 변환한다.\n",
        "\n",
        "        attention_mask = create_attention_mask_from_input_mask(\n",
        "            input_ids, input_mask)\n",
        "\n",
        "        # Run the stacked transformer.\n",
        "        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n",
        "        self.all_encoder_layers = transformer_model(\n",
        "            input_tensor=self.embedding_output,\n",
        "            attention_mask=attention_mask,\n",
        "            hidden_size=config.hidden_size,\n",
        "            num_hidden_layers=config.num_hidden_layers,\n",
        "            num_attention_heads=config.num_attention_heads,\n",
        "            intermediate_size=config.intermediate_size,\n",
        "            intermediate_act_fn=get_activation(config.hidden_act),\n",
        "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
        "            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n",
        "            initializer_range=config.initializer_range,\n",
        "            do_return_all_layers=True)\n",
        "\n",
        "      self.sequence_output = self.all_encoder_layers[-1]\n",
        "      # The \"pooler\" converts the encoded sequence tensor of shape\n",
        "      # [batch_size, seq_length, hidden_size] to a tensor of shape\n",
        "      # [batch_size, hidden_size]. This is necessary for segment-level\n",
        "      # (or segment-pair-level) classification tasks where we need a fixed\n",
        "      # dimensional representation of the segment.\n",
        "      with tf.variable_scope(\"pooler\"):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding to the first token. We assume that this has been pre-trained\n",
        "        # 우리는 단순히 첫 번째 토큰에 해당하는 숨겨진 상태를 취함으로써 모델을 \"풀\"한다. 우리는 이것이 사전 교육을 받은 것으로 가정한다.\n",
        "        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n",
        "        self.pooled_output = tf.layers.dense(\n",
        "            first_token_tensor,\n",
        "            config.hidden_size,\n",
        "            activation=tf.tanh,\n",
        "            kernel_initializer=create_initializer(config.initializer_range))\n",
        "\n",
        "  def get_pooled_output(self):\n",
        "    return self.pooled_output\n",
        "\n",
        "  def get_sequence_output(self):\n",
        "    \"\"\"Gets final hidden layer of encoder. 인코더의 최종 은닉 도면층을 가져옵니다.\n",
        "    Returns:\n",
        "      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n",
        "      to the final hidden of the transformer encoder.\n",
        "      변압기 인코더의 최종 은닉에 해당하는 형상[batch_size, seq_length, hidden_size]의 플로트 텐서\n",
        "    \"\"\"\n",
        "    return self.sequence_output\n",
        "\n",
        "  def get_all_encoder_layers(self):\n",
        "    return self.all_encoder_layers\n",
        "\n",
        "  def get_embedding_output(self):\n",
        "    \"\"\"Gets output of the embedding lookup (i.e., input to the transformer). 내장 조회(즉, 변압기에 대한 입력)의 출력을 가져옵니다.\n",
        "    Returns:\n",
        "      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding to the output of the embedding layer, after summing the word embeddings with the positional embeddings and the token type embeddings, then performing layer normalization. This is the input to the transformer.\n",
        "      위치 임베딩 및 토큰 유형 임베딩과 단어 임베딩을 합친 후 임베딩 레이어의 출력에 해당하는 형상 [batch_size, seq_length, hidden_size]의 플로트 텐서. 이것은 변압기의 입력입니다.\n",
        "    \"\"\"\n",
        "    return self.embedding_output\n",
        "\n",
        "  def get_embedding_table(self):\n",
        "    return self.embedding_table"
      ],
      "metadata": {
        "id": "JRY78xhQEbG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gelu(x):\n",
        "  \"\"\"Gaussian Error Linear Unit. 가우스 오차 선형 유닛\n",
        "  This is a smoother version of the RELU. RELU의 보다 부드러운 버전입니다.\n",
        "  Original paper: https://arxiv.org/abs/1606.08415\n",
        "  Args:\n",
        "    x: float Tensor to perform activation.\n",
        "  Returns:\n",
        "    `x` with the GELU activation applied.\n",
        "  \"\"\"\n",
        "  cdf = 0.5 * (1.0 + tf.tanh(\n",
        "      (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
        "  return x * cdf\n",
        "\n",
        "\n",
        "def get_activation(activation_string):\n",
        "  \"\"\"Maps a string to a Python function, e.g., \"relu\" => `tf.nn.relu`.\n",
        "  Args:\n",
        "    activation_string: String name of the activation function.\n",
        "  Returns:\n",
        "    A Python function corresponding to the activation function. If\n",
        "    `activation_string` is None, empty, or \"linear\", this will return None.\n",
        "    If `activation_string` is not a string, it will return `activation_string`.\n",
        "    \n",
        "    활성화 함수에 해당하는 Python 함수입니다. 한다면\n",
        "    'activation_string'이 None, empty 또는 \"linear\"이며 None을 반환합니다.\n",
        "    'activation_string'이 문자열이 아닌 경우 'activation_string'을 반환합니다.\n",
        "  Raises:\n",
        "    ValueError: The `activation_string` does not correspond to a known\n",
        "      activation.\n",
        "  \"\"\"\n",
        "\n",
        "  # We assume that anything that\"s not a string is already an activation function, so we just return it. \n",
        "  # 문자열이 아닌 것은 이미 활성화 함수라고 가정하여 반환한다.\n",
        "  if not isinstance(activation_string, six.string_types):\n",
        "    return activation_string\n",
        "\n",
        "  if not activation_string:\n",
        "    return None\n",
        "\n",
        "  act = activation_string.lower()\n",
        "  if act == \"linear\":\n",
        "    return None\n",
        "  elif act == \"relu\":\n",
        "    return tf.nn.relu\n",
        "  elif act == \"gelu\":\n",
        "    return gelu\n",
        "  elif act == \"tanh\":\n",
        "    return tf.tanh\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported activation: %s\" % act)\n",
        "\n",
        "\n",
        "def get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n",
        "  \"\"\"Compute the union of the current variables and checkpoint variables. 현재 변수와 체크포인트 변수의 조합을 계산합니다.\"\"\"\n",
        "  assignment_map = {}\n",
        "  initialized_variable_names = {}\n",
        "\n",
        "  name_to_variable = collections.OrderedDict()\n",
        "  for var in tvars:\n",
        "    name = var.name\n",
        "    m = re.match(\"^(.*):\\\\d+$\", name)\n",
        "    if m is not None:\n",
        "      name = m.group(1)\n",
        "    name_to_variable[name] = var\n",
        "\n",
        "  init_vars = tf.train.list_variables(init_checkpoint)\n",
        "\n",
        "  assignment_map = collections.OrderedDict()\n",
        "  for x in init_vars:\n",
        "    (name, var) = (x[0], x[1])\n",
        "    if name not in name_to_variable:\n",
        "      continue\n",
        "    assignment_map[name] = name\n",
        "    initialized_variable_names[name] = 1\n",
        "    initialized_variable_names[name + \":0\"] = 1\n",
        "\n",
        "  return (assignment_map, initialized_variable_names)\n",
        "\n",
        "\n",
        "def dropout(input_tensor, dropout_prob):\n",
        "  \"\"\"Perform dropout.\n",
        "  Args:\n",
        "    input_tensor: float Tensor.\n",
        "    dropout_prob: Python float. The probability of dropping out a value (NOT of\n",
        "      *keeping* a dimension as in `tf.nn.dropout`).\n",
        "  Returns:\n",
        "    A version of `input_tensor` with dropout applied.\n",
        "  \"\"\"\n",
        "  if dropout_prob is None or dropout_prob == 0.0:\n",
        "    return input_tensor\n",
        "\n",
        "  output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n",
        "  return output\n",
        "\n",
        "\n",
        "def layer_norm(input_tensor, name=None):\n",
        "  \"\"\"Run layer normalization on the last dimension of the tensor. 텐서의 마지막 차원에서 레이어 정규화를 실행합니다.\"\"\"\n",
        "  return tf.contrib.layers.layer_norm(\n",
        "      inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)\n",
        "\n",
        "\n",
        "def layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\n",
        "  \"\"\"Runs layer normalization followed by dropout. 계층 정규화에 이어 드롭아웃을 실행합니다\"\"\"\n",
        "  output_tensor = layer_norm(input_tensor, name)\n",
        "  output_tensor = dropout(output_tensor, dropout_prob)\n",
        "  return output_tensor\n",
        "\n",
        "\n",
        "def create_initializer(initializer_range=0.02):\n",
        "  \"\"\"Creates a `truncated_normal_initializer` with the given range. 지정된 범위를 사용하여 'truncated_normal_initializer'를 만듭니다.\"\"\"\n",
        "  return tf.truncated_normal_initializer(stddev=initializer_range)\n",
        "\n",
        "\n",
        "def embedding_lookup(input_ids,\n",
        "                     vocab_size,\n",
        "                     embedding_size=128,\n",
        "                     initializer_range=0.02,\n",
        "                     word_embedding_name=\"word_embeddings\",\n",
        "                     use_one_hot_embeddings=False):\n",
        "  \"\"\"Looks up words embeddings for id tensor.\n",
        "  Args:\n",
        "    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word ids.\n",
        "    vocab_size: int. Size of the embedding vocabulary.\n",
        "    embedding_size: int. Width of the word embeddings.\n",
        "    initializer_range: float. Embedding initialization range.\n",
        "    word_embedding_name: string. Name of the embedding table.\n",
        "    use_one_hot_embeddings: bool. If True, use one-hot method for word embeddings. If False, use `tf.gather()`.\n",
        "\n",
        "    input_ids: 단어 ID를 포함하는 모양 [batch_size, seq_length]의 int32 텐서.\n",
        "    vocab_size: int. 내장 어휘의 크기입니다.\n",
        "    embedding_size: int. 단어 임베딩의 너비입니다.\n",
        "    initializer_range: 부동. 초기화 범위를 내장하고 있습니다.\n",
        "    word_embedding_name: 문자열입니다. 내장 테이블의 이름입니다.\n",
        "    use_one_hot_curdings: bool. True이면 단어 임베딩에 원핫 메소드를 사용합니다. False인 경우 'tf.gather()'를 사용합니다.\n",
        "  Returns:\n",
        "    float Tensor of shape [batch_size, seq_length, embedding_size].\n",
        "  \"\"\"\n",
        "  # This function assumes that the input is of shape [batch_size, seq_length, num_inputs].\n",
        "  # If the input is a 2D tensor of shape [batch_size, seq_length], we reshape to [batch_size, seq_length, 1].\n",
        "  # 이 함수는 입력이 [batch_size, seq_length, num_inputs] 모양이라고 가정한다.\n",
        "  # 입력이 [batch_size, seq_length] 모양의 2D 텐서이면 [batch_size, seq_length, 1]로 모양을 바꾼다.\n",
        "\n",
        "  if input_ids.shape.ndims == 2:\n",
        "    input_ids = tf.expand_dims(input_ids, axis=[-1])\n",
        "\n",
        "  embedding_table = tf.get_variable(\n",
        "      name=word_embedding_name,\n",
        "      shape=[vocab_size, embedding_size],\n",
        "      initializer=create_initializer(initializer_range))\n",
        "\n",
        "  flat_input_ids = tf.reshape(input_ids, [-1])\n",
        "  if use_one_hot_embeddings:\n",
        "    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n",
        "    output = tf.matmul(one_hot_input_ids, embedding_table)\n",
        "  else:\n",
        "    output = tf.gather(embedding_table, flat_input_ids)\n",
        "\n",
        "  input_shape = get_shape_list(input_ids)\n",
        "\n",
        "  output = tf.reshape(output,\n",
        "                      input_shape[0:-1] + [input_shape[-1] * embedding_size])\n",
        "  return (output, embedding_table)\n",
        "\n",
        "\n",
        "def embedding_postprocessor(input_tensor,\n",
        "                            use_token_type=False,\n",
        "                            token_type_ids=None,\n",
        "                            token_type_vocab_size=16,\n",
        "                            token_type_embedding_name=\"token_type_embeddings\",\n",
        "                            use_position_embeddings=True,\n",
        "                            position_embedding_name=\"position_embeddings\",\n",
        "                            initializer_range=0.02,\n",
        "                            max_position_embeddings=512,\n",
        "                            dropout_prob=0.1):\n",
        "  \"\"\"Performs various post-processing on a word embedding tensor.\n",
        "  Args:\n",
        "    input_tensor: float Tensor of shape [batch_size, seq_length, embedding_size].\n",
        "    use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n",
        "    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length]. Must be specified if `use_token_type` is True.\n",
        "    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.\n",
        "    token_type_embedding_name: string. The name of the embedding table variable for token type ids.\n",
        "    use_position_embeddings: bool. Whether to add position embeddings for the position of each token in the sequence.\n",
        "    position_embedding_name: string. The name of the embedding table variable for positional embeddings.\n",
        "    initializer_range: float. Range of the weight initialization.\n",
        "    max_position_embeddings: int. Maximum sequence length that might ever be used with this model. This can be longer than the sequence length of input_tensor, but cannot be shorter.\n",
        "    dropout_prob: float. Dropout probability applied to the final output tensor.\n",
        "\n",
        "    input_tensor: 형상의 부동 텐서 [batch_size, seq_length, medding_size].\n",
        "    use_bul_type: bool. 'token_type_ids'에 대한 임베딩을 추가할지 여부입니다.\n",
        "    token_type_ids: (옵션) 셰이프의 int32 텐서 [batch_size, seq_length]. use_token_type'이 True이면 지정해야 합니다.\n",
        "    token_type_vocab_size: int. 'token_type_ids'의 어휘 크기입니다.\n",
        "    token_type_time_name: 문자열입니다. 토큰 형식 ID에 대한 내장 테이블 변수의 이름입니다.\n",
        "    use_position_clindings: bool. 시퀀스에 있는 각 토큰의 위치에 대한 위치 임베딩을 추가할지 여부입니다.\n",
        "    position_position_name: 문자열입니다. 위치 임베딩을 위한 임베딩 테이블 변수의 이름입니다.\n",
        "    initializer_range: 부동. 가중치 초기화 범위입니다.\n",
        "    max_position_embedding: int. 이 모델에 사용될 수 있는 최대 시퀀스 길이. 이것은 input_tensor의 시퀀스 길이보다 길 수 있지만 더 짧을 수는 없다.\n",
        "    dropout_prob: 부동. 최종 출력 텐서에 적용되는 드롭아웃 확률입니다.\n",
        "  Returns:\n",
        "    float tensor with same shape as `input_tensor`.\n",
        "  Raises:\n",
        "    ValueError: One of the tensor shapes or input values is invalid.\n",
        "  \"\"\"\n",
        "  input_shape = get_shape_list(input_tensor, expected_rank=3)\n",
        "  batch_size = input_shape[0]\n",
        "  seq_length = input_shape[1]\n",
        "  width = input_shape[2]\n",
        "\n",
        "  output = input_tensor\n",
        "\n",
        "  if use_token_type:\n",
        "    if token_type_ids is None:\n",
        "      raise ValueError(\"`token_type_ids` must be specified if\"\n",
        "                       \"`use_token_type` is True.\")\n",
        "    token_type_table = tf.get_variable(\n",
        "        name=token_type_embedding_name,\n",
        "        shape=[token_type_vocab_size, width],\n",
        "        initializer=create_initializer(initializer_range))\n",
        "    # This vocab will be small so we always do one-hot here, since it is always faster for a small vocabulary.\n",
        "    # 이 어휘는 작아서 항상 여기서 원핫을 하는데, 작은 어휘는 항상 빠르기 때문입니다.\n",
        "    flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n",
        "    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)\n",
        "    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n",
        "    token_type_embeddings = tf.reshape(token_type_embeddings,\n",
        "                                       [batch_size, seq_length, width])\n",
        "    output += token_type_embeddings\n",
        "\n",
        "  if use_position_embeddings:\n",
        "    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)\n",
        "    with tf.control_dependencies([assert_op]):\n",
        "      full_position_embeddings = tf.get_variable(\n",
        "          name=position_embedding_name,\n",
        "          shape=[max_position_embeddings, width],\n",
        "          initializer=create_initializer(initializer_range))\n",
        "      # Since the position embedding table is a learned variable, we create it using a (long) sequence length `max_position_embeddings`. The actual sequence length might be shorter than this, for faster training of tasks that do not have long sequences.\n",
        "      # So `full_position_embeddings` is effectively an embedding table for position [0, 1, 2, ..., max_position_embeddings-1], and the current sequence has positions [0, 1, 2, ... seq_length-1], so we can just perform a slice.\n",
        "      # 위치 임베딩 테이블은 학습된 변수이기 때문에 우리는 (긴) 시퀀스 길이 'max_position_embedding'을 사용하여 테이블을 만든다. 실제 시퀀스 길이는 긴 시퀀스가 없는 작업의 빠른 훈련을 위해 이보다 짧을 수 있다. \n",
        "      #따라서 'full_position_embeddings'는 위치 [0, 1, 2, ..., max_position_embeddings-1]에 대한 효과적인 내장 테이블이며, 현재 시퀀스는 위치 [0, 1, 2, ... seq_length-1]를 가지고 있으므로 슬라이스를 수행할 수 있다.\n",
        "      position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n",
        "                                     [seq_length, -1])\n",
        "      num_dims = len(output.shape.as_list())\n",
        "\n",
        "      # Only the last two dimensions are relevant (`seq_length` and `width`), so we broadcast among the first dimensions, which is typically just the batch size.\n",
        "      # 마지막 두 차원만 관련이 있으므로('seq_length'와 'width'), 일반적으로 배치 크기인 첫 번째 차원 사이에서 브로드캐스트합니다.\n",
        "      position_broadcast_shape = []\n",
        "      for _ in range(num_dims - 2):\n",
        "        position_broadcast_shape.append(1)\n",
        "      position_broadcast_shape.extend([seq_length, width])\n",
        "      position_embeddings = tf.reshape(position_embeddings,\n",
        "                                       position_broadcast_shape)\n",
        "      output += position_embeddings\n",
        "\n",
        "  output = layer_norm_and_dropout(output, dropout_prob)\n",
        "  return output\n",
        "\n",
        "\n",
        "def create_attention_mask_from_input_mask(from_tensor, to_mask):\n",
        "  \"\"\"Create 3D attention mask from a 2D tensor mask.\n",
        "  Args:\n",
        "    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n",
        "    to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n",
        "  Returns:\n",
        "    float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n",
        "  \"\"\"\n",
        "  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
        "  batch_size = from_shape[0]\n",
        "  from_seq_length = from_shape[1]\n",
        "\n",
        "  to_shape = get_shape_list(to_mask, expected_rank=2)\n",
        "  to_seq_length = to_shape[1]\n",
        "\n",
        "  to_mask = tf.cast(\n",
        "      tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n",
        "\n",
        "  # We don't assume that `from_tensor` is a mask (although it could be). We don't actually care if we attend *from* padding tokens (only *to* padding) tokens so we create a tensor of all ones. `broadcast_ones` = [batch_size, from_seq_length, 1]\n",
        "  # 우리는 from_tensor가 마스크라고 가정하지 않는다. *from* 패딩 토큰 (*to* 패딩만)에 참석하는지 여부는 중요하지 않으므로 모든 토큰의 텐서를 만듭니다. 'self_ones' = [size_size, from_seq_length, 1]\n",
        "  broadcast_ones = tf.ones(\n",
        "      shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n",
        "\n",
        "  # Here we broadcast along two dimensions to create the mask.\n",
        "  # 여기서는 마스크를 만들기 위해 2차원을 따라 방송합니다.\n",
        "  mask = broadcast_ones * to_mask\n",
        "\n",
        "  return mask\n",
        "\n",
        "\n",
        "def attention_layer(from_tensor,\n",
        "                    to_tensor,\n",
        "                    attention_mask=None,\n",
        "                    num_attention_heads=1,\n",
        "                    size_per_head=512,\n",
        "                    query_act=None,\n",
        "                    key_act=None,\n",
        "                    value_act=None,\n",
        "                    attention_probs_dropout_prob=0.0,\n",
        "                    initializer_range=0.02,\n",
        "                    do_return_2d_tensor=False,\n",
        "                    batch_size=None,\n",
        "                    from_seq_length=None,\n",
        "                    to_seq_length=None):\n",
        "  \"\"\"Performs multi-headed attention from `from_tensor` to `to_tensor`.\n",
        "  This is an implementation of multi-headed attention based on \"Attention\n",
        "  is all you Need\". If `from_tensor` and `to_tensor` are the same, then\n",
        "  this is self-attention. Each timestep in `from_tensor` attends to the\n",
        "  corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n",
        "  This function first projects `from_tensor` into a \"query\" tensor and\n",
        "  `to_tensor` into \"key\" and \"value\" tensors. These are (effectively) a list\n",
        "  of tensors of length `num_attention_heads`, where each tensor is of shape\n",
        "  [batch_size, seq_length, size_per_head].\n",
        "  Then, the query and key tensors are dot-producted and scaled. These are\n",
        "  softmaxed to obtain attention probabilities. The value tensors are then\n",
        "  interpolated by these probabilities, then concatenated back to a single\n",
        "  tensor and returned.\n",
        "  In practice, the multi-headed attention are done with transposes and\n",
        "  reshapes rather than actual separate tensors.\n",
        "\n",
        "  from_tensor에서 to_tensor까지 멀티 헤드 어텐션을 수행합니다. 이것은 \"관심만 있으면 된다\"를 기반으로 한 다자주의 구현입니다. from_tensor와 to_tensor가 같다면 이는 자기주의다. \n",
        "  from_tensor'의 각 시간 단계는 'to_tensor'의 해당 시퀀스에 참여하고 벡터가 있는 고정된 값을 반환합니다. \n",
        "  이 함수는 먼저 from_tensor를 쿼리 텐서로, to_tensor를 키 텐서와 값 텐서로 투영한다. 이것들은 (효과적으로) 길이가 'num_attention_heads'인 텐서의 목록이며, \n",
        "  여기서 각 텐서는 모양 [batch_size, seq_length, size_per_head]이다. 그런 다음 쿼리와 키 텐서가 점으로 생성되고 크기가 조정됩니다. 이것들은 주의 확률을 얻기 위해 소프트맥스된다. \n",
        "  그런 다음 값 텐서는 이러한 확률에 의해 보간되고, 다시 단일 텐서에 연결되고 반환된다. 실제로, 다중 헤드 주의는 실제 개별 텐서가 아닌 전치 및 재형식으로 수행됩니다.\n",
        "\n",
        "  Args:\n",
        "    from_tensor: float Tensor of shape [batch_size, from_seq_length, from_width].\n",
        "    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n",
        "    attention_mask: (optional) int32 Tensor of shape [batch_size, from_seq_length, to_seq_length]. The values should be 1 or 0. The attention scores will effectively be set to -infinity for any positions in the mask that are 0, and will be unchanged for positions that are 1.\n",
        "    num_attention_heads: int. Number of attention heads.\n",
        "    size_per_head: int. Size of each attention head.\n",
        "    query_act: (optional) Activation function for the query transform.\n",
        "    key_act: (optional) Activation function for the key transform.\n",
        "    value_act: (optional) Activation function for the value transform.\n",
        "    attention_probs_dropout_prob: (optional) float. Dropout probability of the attention probabilities.\n",
        "    initializer_range: float. Range of the weight initializer.\n",
        "    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size * from_seq_length, num_attention_heads * size_per_head]. If False, the output will be of shape [batch_size, from_seq_length, num_attention_heads * size_per_head].\n",
        "    batch_size: (Optional) int. If the input is 2D, this might be the batch size of the 3D version of the `from_tensor` and `to_tensor`.\n",
        "    from_seq_length: (Optional) If the input is 2D, this might be the seq length of the 3D version of the `from_tensor`. to_seq_length: (Optional) If the input is 2D, this might be the seq length of the 3D version of the `to_tensor`.\n",
        "  Returns:\n",
        "    float Tensor of shape [batch_size, from_seq_length,\n",
        "      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is\n",
        "      true, this will be of shape [batch_size * from_seq_length,\n",
        "      num_attention_heads * size_per_head]).\n",
        "  Raises:\n",
        "    ValueError: Any of the arguments or tensor shapes are invalid.\n",
        "  \"\"\"\n",
        "\n",
        "  def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n",
        "                           seq_length, width):\n",
        "    output_tensor = tf.reshape(\n",
        "        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n",
        "\n",
        "    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n",
        "    return output_tensor\n",
        "\n",
        "  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
        "  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])\n",
        "\n",
        "  if len(from_shape) != len(to_shape):\n",
        "    raise ValueError(\n",
        "        \"The rank of `from_tensor` must match the rank of `to_tensor`.\")\n",
        "\n",
        "  if len(from_shape) == 3:\n",
        "    batch_size = from_shape[0]\n",
        "    from_seq_length = from_shape[1]\n",
        "    to_seq_length = to_shape[1]\n",
        "  elif len(from_shape) == 2:\n",
        "    if (batch_size is None or from_seq_length is None or to_seq_length is None):\n",
        "      raise ValueError(\n",
        "          \"When passing in rank 2 tensors to attention_layer, the values \"\n",
        "          \"for `batch_size`, `from_seq_length`, and `to_seq_length` \"\n",
        "          \"must all be specified.\")\n",
        "\n",
        "  # Scalar dimensions referenced here:\n",
        "  #   B = batch size (number of sequences)\n",
        "  #   F = `from_tensor` sequence length\n",
        "  #   T = `to_tensor` sequence length\n",
        "  #   N = `num_attention_heads`\n",
        "  #   H = `size_per_head`\n",
        "\n",
        "  from_tensor_2d = reshape_to_matrix(from_tensor)\n",
        "  to_tensor_2d = reshape_to_matrix(to_tensor)\n",
        "\n",
        "  # `query_layer` = [B*F, N*H]\n",
        "  query_layer = tf.layers.dense(\n",
        "      from_tensor_2d,\n",
        "      num_attention_heads * size_per_head,\n",
        "      activation=query_act,\n",
        "      name=\"query\",\n",
        "      kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "  # `key_layer` = [B*T, N*H]\n",
        "  key_layer = tf.layers.dense(\n",
        "      to_tensor_2d,\n",
        "      num_attention_heads * size_per_head,\n",
        "      activation=key_act,\n",
        "      name=\"key\",\n",
        "      kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "  # `value_layer` = [B*T, N*H]\n",
        "  value_layer = tf.layers.dense(\n",
        "      to_tensor_2d,\n",
        "      num_attention_heads * size_per_head,\n",
        "      activation=value_act,\n",
        "      name=\"value\",\n",
        "      kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "  # `query_layer` = [B, N, F, H]\n",
        "  query_layer = transpose_for_scores(query_layer, batch_size,\n",
        "                                     num_attention_heads, from_seq_length,\n",
        "                                     size_per_head)\n",
        "\n",
        "  # `key_layer` = [B, N, T, H]\n",
        "  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,\n",
        "                                   to_seq_length, size_per_head)\n",
        "\n",
        "  # Take the dot product between \"query\" and \"key\" to get the raw attention scores. `attention_scores` = [B, N, F, T]\n",
        "  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n",
        "  attention_scores = tf.multiply(attention_scores, 1.0 / math.sqrt(float(size_per_head)))\n",
        "\n",
        "  if attention_mask is not None:\n",
        "    # `attention_mask` = [B, 1, F, T]\n",
        "    attention_mask = tf.expand_dims(attention_mask, axis=[1])\n",
        "\n",
        "    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for masked positions, this operation will create a tensor which is 0.0 for positions we want to attend and -10000.0 for masked positions.\n",
        "    # attention_mask는 참석하려는 위치의 경우 1.0이고 마스킹된 위치의 경우 0.0이므로, 이 연산은 참석하려는 위치의 경우 0.0이고 마스킹된 위치의 경우 -10000.0인 텐서를 생성한다.\n",
        "    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n",
        "\n",
        "    # Since we are adding it to the raw scores before the softmax, this is effectively the same as removing these entirely.\n",
        "    # 소프트맥스 이전의 원시 점수에 추가하기 때문에 사실상 이것들을 전부 제거하는 것과 같습니다.\n",
        "    attention_scores += adder\n",
        "\n",
        "  # Normalize the attention scores to probabilities. `attention_probs` = [B, N, F, T]\n",
        "  attention_probs = tf.nn.softmax(attention_scores)\n",
        "\n",
        "  # This is actually dropping out entire tokens to attend to, which might seem a bit unusual, but is taken from the original Transformer paper.\n",
        "  # 이것은 사실 참석하기 위해 토큰을 통째로 내보내는 것인데, 이것은 약간 특이해 보일 수 있지만 원래의 트랜스포머 종이에서 따온 것이다.\n",
        "  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)\n",
        "\n",
        "  # `value_layer` = [B, T, N, H]\n",
        "  value_layer = tf.reshape(\n",
        "      value_layer,\n",
        "      [batch_size, to_seq_length, num_attention_heads, size_per_head])\n",
        "\n",
        "  # `value_layer` = [B, N, T, H]\n",
        "  value_layer = tf.transpose(value_layer, [0, 2, 1, 3])\n",
        "\n",
        "  # `context_layer` = [B, N, F, H]\n",
        "  context_layer = tf.matmul(attention_probs, value_layer)\n",
        "\n",
        "  # `context_layer` = [B, F, N, H]\n",
        "  context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n",
        "\n",
        "  if do_return_2d_tensor:\n",
        "    # `context_layer` = [B*F, N*H]\n",
        "    context_layer = tf.reshape(\n",
        "        context_layer,\n",
        "        [batch_size * from_seq_length, num_attention_heads * size_per_head])\n",
        "  else:\n",
        "    # `context_layer` = [B, F, N*H]\n",
        "    context_layer = tf.reshape(\n",
        "        context_layer,\n",
        "        [batch_size, from_seq_length, num_attention_heads * size_per_head])\n",
        "\n",
        "  return context_layer\n",
        "\n",
        "\n",
        "def transformer_model(input_tensor,\n",
        "                      attention_mask=None,\n",
        "                      hidden_size=768,\n",
        "                      num_hidden_layers=12,\n",
        "                      num_attention_heads=12,\n",
        "                      intermediate_size=3072,\n",
        "                      intermediate_act_fn=gelu,\n",
        "                      hidden_dropout_prob=0.1,\n",
        "                      attention_probs_dropout_prob=0.1,\n",
        "                      initializer_range=0.02,\n",
        "                      do_return_all_layers=False):\n",
        "  \"\"\"Multi-headed, multi-layer Transformer from \"Attention is All You Need\".\n",
        "  This is almost an exact implementation of the original Transformer encoder.\n",
        "  See the original paper:\n",
        "  https://arxiv.org/abs/1706.03762\n",
        "  Also see:\n",
        "  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n",
        "  Args:\n",
        "    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n",
        "    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n",
        "      seq_length], with 1 for positions that can be attended to and 0 in\n",
        "      positions that should not be.\n",
        "    hidden_size: int. Hidden size of the Transformer.\n",
        "    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n",
        "    num_attention_heads: int. Number of attention heads in the Transformer.\n",
        "    intermediate_size: int. The size of the \"intermediate\" (a.k.a., feed\n",
        "      forward) layer.\n",
        "    intermediate_act_fn: function. The non-linear activation function to apply\n",
        "      to the output of the intermediate/feed-forward layer.\n",
        "    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n",
        "    attention_probs_dropout_prob: float. Dropout probability of the attention\n",
        "      probabilities.\n",
        "    initializer_range: float. Range of the initializer (stddev of truncated\n",
        "      normal).\n",
        "    do_return_all_layers: Whether to also return all layers or just the final\n",
        "      layer.\n",
        "  Returns:\n",
        "    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n",
        "    hidden layer of the Transformer.\n",
        "  Raises:\n",
        "    ValueError: A Tensor shape or parameter is invalid.\n",
        "  \"\"\"\n",
        "  if hidden_size % num_attention_heads != 0:\n",
        "    raise ValueError(\n",
        "        \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "        \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
        "\n",
        "  attention_head_size = int(hidden_size / num_attention_heads)\n",
        "  input_shape = get_shape_list(input_tensor, expected_rank=3)\n",
        "  batch_size = input_shape[0]\n",
        "  seq_length = input_shape[1]\n",
        "  input_width = input_shape[2]\n",
        "\n",
        "  # The Transformer performs sum residuals on all layers so the input needs to be the same as the hidden size.\n",
        "  # 변압기는 모든 도면층에서 잔차 합을 수행하므로 입력이 숨겨진 크기와 같아야 합니다.\n",
        "  if input_width != hidden_size:\n",
        "    raise ValueError(\"The width of the input tensor (%d) != hidden size (%d)\" %\n",
        "                     (input_width, hidden_size))\n",
        "\n",
        "  # We keep the representation as a 2D tensor to avoid re-shaping it back and forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on the GPU/CPU but may not be free on the TPU, so we want to minimize them to help the optimizer.\n",
        "  # 우리는 표현을 2D 텐서에서 2D 텐서로 앞뒤로 재형성하지 않도록 2D 텐서로 유지한다. 재모양은 일반적으로 GPU/CPU에서 사용 가능하지만 TPU에서는 사용 불가능할 수 있으므로 최적기에 도움이 되도록 최소화하고자 합니다.\n",
        "  prev_output = reshape_to_matrix(input_tensor)\n",
        "\n",
        "  all_layer_outputs = []\n",
        "  for layer_idx in range(num_hidden_layers):\n",
        "    with tf.variable_scope(\"layer_%d\" % layer_idx):\n",
        "      layer_input = prev_output\n",
        "\n",
        "      with tf.variable_scope(\"attention\"):\n",
        "        attention_heads = []\n",
        "        with tf.variable_scope(\"self\"):\n",
        "          attention_head = attention_layer(\n",
        "              from_tensor=layer_input,\n",
        "              to_tensor=layer_input,\n",
        "              attention_mask=attention_mask,\n",
        "              num_attention_heads=num_attention_heads,\n",
        "              size_per_head=attention_head_size,\n",
        "              attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
        "              initializer_range=initializer_range,\n",
        "              do_return_2d_tensor=True,\n",
        "              batch_size=batch_size,\n",
        "              from_seq_length=seq_length,\n",
        "              to_seq_length=seq_length)\n",
        "          attention_heads.append(attention_head)\n",
        "\n",
        "        attention_output = None\n",
        "        if len(attention_heads) == 1:\n",
        "          attention_output = attention_heads[0]\n",
        "        else:\n",
        "          # In the case where we have other sequences, we just concatenate them to the self-attention head before the projection.\n",
        "          # 다른 시퀀스가 있는 경우 투영 전에 자기 주의 헤드에 연결하기만 하면 됩니다.\n",
        "          attention_output = tf.concat(attention_heads, axis=-1)\n",
        "\n",
        "        # Run a linear projection of `hidden_size` then add a residual with `layer_input`.\n",
        "        # hidden_size'의 선형 투영을 실행한 다음 layer_input'로 잔차를 추가한다.\n",
        "        with tf.variable_scope(\"output\"):\n",
        "          attention_output = tf.layers.dense(\n",
        "              attention_output,\n",
        "              hidden_size,\n",
        "              kernel_initializer=create_initializer(initializer_range))\n",
        "          attention_output = dropout(attention_output, hidden_dropout_prob)\n",
        "          attention_output = layer_norm(attention_output + layer_input)\n",
        "\n",
        "      # The activation is only applied to the \"intermediate\" hidden layer.\n",
        "      # 활성화는 \"중간\" 은닉 계층에만 적용됩니다.\n",
        "      with tf.variable_scope(\"intermediate\"):\n",
        "        intermediate_output = tf.layers.dense(\n",
        "            attention_output,\n",
        "            intermediate_size,\n",
        "            activation=intermediate_act_fn,\n",
        "            kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "      # Down-project back to `hidden_size` then add the residual.\n",
        "      # 'hidden_size'로 다시 다운 투영한 다음 나머지를 추가합니다.\n",
        "      with tf.variable_scope(\"output\"):\n",
        "        layer_output = tf.layers.dense(\n",
        "            intermediate_output,\n",
        "            hidden_size,\n",
        "            kernel_initializer=create_initializer(initializer_range))\n",
        "        layer_output = dropout(layer_output, hidden_dropout_prob)\n",
        "        layer_output = layer_norm(layer_output + attention_output)\n",
        "        prev_output = layer_output\n",
        "        all_layer_outputs.append(layer_output)\n",
        "\n",
        "  if do_return_all_layers:\n",
        "    final_outputs = []\n",
        "    for layer_output in all_layer_outputs:\n",
        "      final_output = reshape_from_matrix(layer_output, input_shape)\n",
        "      final_outputs.append(final_output)\n",
        "    return final_outputs\n",
        "  else:\n",
        "    final_output = reshape_from_matrix(prev_output, input_shape)\n",
        "    return final_output\n",
        "\n",
        "\n",
        "def get_shape_list(tensor, expected_rank=None, name=None):\n",
        "  \"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n",
        "  Args:\n",
        "    tensor: A tf.Tensor object to find the shape of.\n",
        "    expected_rank: (optional) int. The expected rank of `tensor`. If this is specified and the `tensor` has a different rank, and exception will be thrown.\n",
        "    name: Optional name of the tensor for the error message.\n",
        "  Returns:\n",
        "    A list of dimensions of the shape of tensor. All static dimensions will be returned as python integers, and dynamic dimensions will be returned as tf.Tensor scalars.\n",
        "    텐서 모양의 차원 목록입니다. 모든 정적 치수는 파이썬 정수로 반환되고 동적 치수는 tf로 반환됩니다.텐서 스칼라.\n",
        "  \"\"\"\n",
        "  if name is None:\n",
        "    name = tensor.name\n",
        "\n",
        "  if expected_rank is not None:\n",
        "    assert_rank(tensor, expected_rank, name)\n",
        "\n",
        "  shape = tensor.shape.as_list()\n",
        "\n",
        "  non_static_indexes = []\n",
        "  for (index, dim) in enumerate(shape):\n",
        "    if dim is None:\n",
        "      non_static_indexes.append(index)\n",
        "\n",
        "  if not non_static_indexes:\n",
        "    return shape\n",
        "\n",
        "  dyn_shape = tf.shape(tensor)\n",
        "  for index in non_static_indexes:\n",
        "    shape[index] = dyn_shape[index]\n",
        "  return shape\n",
        "\n",
        "\n",
        "def reshape_to_matrix(input_tensor):\n",
        "  \"\"\"Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).\"\"\"\n",
        "  ndims = input_tensor.shape.ndims\n",
        "  if ndims < 2:\n",
        "    raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n",
        "                     (input_tensor.shape))\n",
        "  if ndims == 2:\n",
        "    return input_tensor\n",
        "\n",
        "  width = input_tensor.shape[-1]\n",
        "  output_tensor = tf.reshape(input_tensor, [-1, width])\n",
        "  return output_tensor\n",
        "\n",
        "\n",
        "def reshape_from_matrix(output_tensor, orig_shape_list):\n",
        "  \"\"\"Reshapes a rank 2 tensor back to its original rank >= 2 tensor.\"\"\"\n",
        "  if len(orig_shape_list) == 2:\n",
        "    return output_tensor\n",
        "\n",
        "  output_shape = get_shape_list(output_tensor)\n",
        "\n",
        "  orig_dims = orig_shape_list[0:-1]\n",
        "  width = output_shape[-1]\n",
        "\n",
        "  return tf.reshape(output_tensor, orig_dims + [width])\n",
        "\n",
        "\n",
        "def assert_rank(tensor, expected_rank, name=None):\n",
        "  \"\"\"Raises an exception if the tensor rank is not of the expected rank.\n",
        "  Args:\n",
        "    tensor: A tf.Tensor to check the rank of.\n",
        "    expected_rank: Python integer or list of integers, expected rank.\n",
        "    name: Optional name of the tensor for the error message.\n",
        "  Raises:\n",
        "    ValueError: If the expected shape doesn't match the actual shape.\n",
        "  \"\"\"\n",
        "  if name is None:\n",
        "    name = tensor.name\n",
        "\n",
        "  expected_rank_dict = {}\n",
        "  if isinstance(expected_rank, six.integer_types):\n",
        "    expected_rank_dict[expected_rank] = True\n",
        "  else:\n",
        "    for x in expected_rank:\n",
        "      expected_rank_dict[x] = True\n",
        "\n",
        "  actual_rank = tensor.shape.ndims\n",
        "  if actual_rank not in expected_rank_dict:\n",
        "    scope_name = tf.get_variable_scope().name\n",
        "    raise ValueError(\n",
        "        \"For the tensor `%s` in scope `%s`, the actual rank \"\n",
        "        \"`%d` (shape = %s) is not equal to the expected rank `%s`\" %\n",
        "        (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))"
      ],
      "metadata": {
        "id": "8unnJKj6Eboz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BgaUNv3lWZuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EzIbmQ-mWZpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "cIQdB3zwWZj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# coding=utf-8\n",
        "# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" Fine-tuning the library models for sequence classification.\"\"\"\n",
        "# You can also adapt this script on your own text classification task. Pointers for this are left as comments.\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "from dataclasses import dataclass, field\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    HfArgumentParser,\n",
        "    PretrainedConfig,\n",
        "    TFAutoModelForSequenceClassification,\n",
        "    TFTrainingArguments,\n",
        "    set_seed,\n",
        ")\n",
        "from transformers.file_utils import CONFIG_NAME, TF2_WEIGHTS_NAME\n",
        "\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"  # Reduce the amount of console output from TF\n",
        "# Tensorflow는 TF_CPP_MIN_LOG_LEVEL 이라는 환경 변수를 통해 로깅을 제어 할 수 있다. \n",
        "# 기본값은 0 (모든 로그가 표시됨)이지만 INFO 로그를 필터링하려면 1, WARNING 로그를 필터링하려면 2, ERROR 로그를 추가로 필터링하려면 3으로 설정할 수 있다.\n",
        "import tensorflow as tf  # noqa: E402\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "# region Helper classes\n",
        "class SavePretrainedCallback(tf.keras.callbacks.Callback):\n",
        "    # Hugging Face models have a save_pretrained() method that saves both the weights and the necessary metadata to allow them to be loaded as a pretrained model in future. This is a simple Keras callback that saves the model with this method after each epoch.\n",
        "    # Hugging Face 모델에는 가중치와 필요한 메타데이터를 모두 저장하는 save_precented() 방법이 있어 나중에 사전 교육된 모델로 로드할 수 있다. 이것은 Epoch 이후에 이 방법으로 모델을 저장하는 간단한 Keras 콜백입니다.\n",
        "    def __init__(self, output_dir, **kwargs):\n",
        "        super().__init__()\n",
        "        self.output_dir = output_dir\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.model.save_pretrained(self.output_dir)\n",
        "\n",
        "\n",
        "def convert_dataset_for_tensorflow(\n",
        "    dataset, non_label_column_names, batch_size, dataset_mode=\"variable_batch\", shuffle=True, drop_remainder=True\n",
        "):\n",
        "    \"\"\"Converts a Hugging Face dataset to a Tensorflow Dataset. The dataset_mode controls whether we pad all batches to the maximum sequence length, or whether we only pad to the maximum length within that batch. The former is most useful when training on TPU, as a new graph compilation is required for each sequence length.\n",
        "    Hugging Face 데이터 세트를 Tensorflow 데이터 세트로 변환합니다. dataset_mode는 모든 배치를 최대 시퀀스 길이로 패딩할지 또는 해당 배치 내에서 최대 길이로만 패딩할지 여부를 제어합니다. 각 시퀀스 길이에 대해 새로운 그래프 컴파일이 필요하기 때문에 전자는 TPU에 대한 훈련 시 가장 유용합니다.\n",
        "    \"\"\"\n",
        "\n",
        "    def densify_ragged_batch(features, label=None):\n",
        "        # 밀도화하다\n",
        "        features = {\n",
        "            feature: ragged_tensor.to_tensor(shape=batch_shape[feature]) for feature, ragged_tensor in features.items()\n",
        "        }\n",
        "        if label is None:\n",
        "            return features\n",
        "        else:\n",
        "            return features, label\n",
        "\n",
        "    feature_keys = list(set(dataset.features.keys()) - set(non_label_column_names + [\"label\"]))\n",
        "    if dataset_mode == \"variable_batch\":\n",
        "        batch_shape = {key: None for key in feature_keys}\n",
        "        data = {key: tf.ragged.constant(dataset[key]) for key in feature_keys}\n",
        "    elif dataset_mode == \"constant_batch\":\n",
        "        data = {key: tf.ragged.constant(dataset[key]) for key in feature_keys}\n",
        "        batch_shape = {\n",
        "            key: tf.concat(([batch_size], ragged_tensor.bounding_shape()[1:]), axis=0)\n",
        "            for key, ragged_tensor in data.items()\n",
        "        }\n",
        "    else:\n",
        "        raise ValueError(\"Unknown dataset mode!\")\n",
        "\n",
        "    if \"label\" in dataset.features:\n",
        "        labels = tf.convert_to_tensor(np.array(dataset[\"label\"]))\n",
        "        tf_dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
        "    else:\n",
        "        tf_dataset = tf.data.Dataset.from_tensor_slices(data)\n",
        "    if shuffle:\n",
        "        tf_dataset = tf_dataset.shuffle(buffer_size=len(dataset))\n",
        "    tf_dataset = tf_dataset.batch(batch_size=batch_size, drop_remainder=drop_remainder).map(densify_ragged_batch)\n",
        "    return tf_dataset\n",
        "\n",
        "\n",
        "# endregion, 지역\n",
        "\n",
        "\n",
        "# region Command-line arguments\n",
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval. Using `HfArgumentParser` we can turn this class into argparse arguments to be able to specify them on the command line.\n",
        "    교육 및 평가를 위해 어떤 데이터를 입력할 것인가에 대한 논쟁입니다. HfArgumentParser'를 사용하여 이 클래스를 argparse 인수로 변환하여 명령행에 지정할 수 있습니다.\n",
        "    \"\"\"\n",
        "    # 데이터 타입 지정 , \n",
        "    # field() : list와 같은 컨테이너 타입의 빈 값을 기본값으로 할당할 때, field 함수를 할당받아 사용해야 함\n",
        "    train_file: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"A csv or a json file containing the training data.\"}\n",
        "    )\n",
        "    validation_file: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"A csv or a json file containing the validation data.\"}\n",
        "    )\n",
        "    test_file: Optional[str] = field(default=None, metadata={\"help\": \"A csv or a json file containing the test data.\"})\n",
        "\n",
        "    max_seq_length: int = field(\n",
        "        default=128,\n",
        "        metadata={\n",
        "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "            \"than this will be truncated, sequences shorter will be padded.\"\n",
        "        },\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n",
        "    )\n",
        "    pad_to_max_length: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n",
        "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n",
        "            \"Data will always be padded when using TPUs.\"\n",
        "        },\n",
        "    )\n",
        "    max_train_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
        "            \"value if set.\"\n",
        "        },\n",
        "    )\n",
        "    max_val_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"For debugging purposes or quicker training, truncate the number of validation examples to this \"\n",
        "            \"value if set.\"\n",
        "        },\n",
        "    )\n",
        "    max_test_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"For debugging purposes or quicker training, truncate the number of test examples to this \"\n",
        "            \"value if set.\"\n",
        "        },\n",
        "    )\n",
        "\n",
        "    def __post_init__(self):\n",
        "        train_extension = self.train_file.split(\".\")[-1].lower() if self.train_file is not None else None\n",
        "        validation_extension = (\n",
        "            self.validation_file.split(\".\")[-1].lower() if self.validation_file is not None else None\n",
        "        )\n",
        "        test_extension = self.test_file.split(\".\")[-1].lower() if self.test_file is not None else None\n",
        "        extensions = {train_extension, validation_extension, test_extension}\n",
        "        extensions.discard(None)\n",
        "        assert len(extensions) != 0, \"Need to supply at least one of --train_file, --validation_file or --test_file!\"\n",
        "        assert len(extensions) == 1, \"All input files should have the same file extension, either csv or json!\"\n",
        "        assert \"csv\" in extensions or \"json\" in extensions, \"Input files should have either .csv or .json extensions!\"\n",
        "        self.input_file_extension = extensions.pop()\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
        "    미세 조정할 모델/구성/토큰나이저와 관련된 인수입니다.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name_or_path: str = field(\n",
        "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
        "    )\n",
        "    config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
        "    )\n",
        "    tokenizer_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
        "    )\n",
        "    model_revision: str = field(\n",
        "        default=\"main\",\n",
        "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
        "    )\n",
        "    use_auth_token: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n",
        "            \"with private models).\"\n",
        "        },\n",
        "    )\n",
        "\n",
        "\n",
        "# endregion\n",
        "\n",
        "\n",
        "def main():\n",
        "    # region Argument parsing See all possible arguments in src/transformers/training_args.py or by passing the --help flag to this script. We now keep distinct sets of args, for a cleaner separation of concerns.\n",
        "    # 영역 인수 구문 분석 src/transformers/training_args.py에서 가능한 모든 인수를 보거나 --help 플래그를 이 스크립트에 전달하여 인수를 확인합니다. 우리는 이제 우려를 더 깨끗하게 분리하기 위해 별개의 arg 세트를 보관하고 있습니다.\n",
        "\n",
        "    # HfArgumentParser클래스는 Huggingface의에서 제공되는 transformers라이브러리입니다. 이 클래스는 하나 이상의 데이터 클래스를 사용하고 해당 데이터 속성을 ArgumentParser개체 의 명령줄 인수로 바꿉\n",
        "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TFTrainingArguments))\n",
        "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
        "        # sys.argv[1] : command 인자값\n",
        "        # If we pass only one argument to the script and it's the path to a json file, let's parse it to get our arguments.\n",
        "        # 스크립트에 인수 하나만 전달하면 json 파일로 가는 경로라면, 인수를 얻기 위해 구문 분석해보자.\n",
        "        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
        "        # os.path.abspath : 특정 경로에 대해 절대 경로 얻기\n",
        "    else:\n",
        "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
        "    output_dir = Path(training_args.output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    # endregion\n",
        "\n",
        "    # region Checkpoints\n",
        "    # Detecting last checkpoint.\n",
        "    checkpoint = None\n",
        "    if len(os.listdir(training_args.output_dir)) > 0 and not training_args.overwrite_output_dir:\n",
        "        if (output_dir / CONFIG_NAME).is_file() and (output_dir / TF2_WEIGHTS_NAME).is_file():\n",
        "            checkpoint = output_dir\n",
        "            logger.info(\n",
        "                f\"Checkpoint detected, resuming training from checkpoint in {training_args.output_dir}. To avoid this\"\n",
        "                \" behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
        "                \"Use --overwrite_output_dir to continue regardless.\"\n",
        "            )\n",
        "\n",
        "    # endregion\n",
        "\n",
        "    # region Logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        handlers=[logging.StreamHandler(sys.stdout)],\n",
        "    )\n",
        "    logger.setLevel(logging.INFO)\n",
        "\n",
        "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
        "    # endregion\n",
        "\n",
        "    # region Loading data For CSV/JSON files, this script will use the 'label' field as the label and the 'sentence1' and optionally 'sentence2' fields as inputs if they exist. If not, the first two fields not named label are used if at least two\n",
        "    # columns are provided. Note that the term 'sentence' can be slightly misleading, as they often contain more than a single grammatical sentence, when the task requires it.\n",
        "    # 데이터 영역 로드 CSV/JSON 파일의 경우 이 스크립트는 'label' 필드를 레이블로 사용하고 'sentence1' 필드와 선택적으로 'sentence2' 필드를 입력으로 사용합니다. \n",
        "    # 그렇지 않은 경우 이름이 지정되지 않은 처음 두 필드는 적어도 두 개의 열이 제공된 경우 사용됩니다. '문장'이라는 용어는 작업이 필요로 할 때 종종 하나의 문법 문장을 포함하기 때문에 약간 오해의 소지가 있을 수 있습니다.\n",
        "    #\n",
        "    # If the CSVs/JSONs contain only one non-label column, the script does single sentence classification on this single column. You can easily tweak this behavior (see below)\n",
        "    # CSV/JSON에 레이블이 아닌 열이 하나만 포함된 경우 스크립트는 이 단일 열에 대해 단일 문장 분류를 수행합니다. 이 동작을 쉽게 조정할 수 있습니다\n",
        "    #\n",
        "    # In distributed training, the load_dataset function guarantee that only one local process can concurrently download the dataset.\n",
        "    # 분산 교육에서 load_dataset 함수는 하나의 로컬 프로세스만 동시에 데이터 집합을 다운로드할 수 있도록 보장합니다.\n",
        "    data_files = {\"train\": data_args.train_file, \"validation\": data_args.validation_file, \"test\": data_args.test_file}\n",
        "    data_files = {key: file for key, file in data_files.items() if file is not None}\n",
        "\n",
        "    for key in data_files.keys():\n",
        "        logger.info(f\"Loading a local file for {key}: {data_files[key]}\")\n",
        "\n",
        "    if data_args.input_file_extension == \"csv\":\n",
        "        # Loading a dataset from local csv files\n",
        "        datasets = load_dataset(\"csv\", data_files=data_files, cache_dir=model_args.cache_dir)\n",
        "    else:\n",
        "        # Loading a dataset from local json files\n",
        "        datasets = load_dataset(\"json\", data_files=data_files, cache_dir=model_args.cache_dir)\n",
        "    # See more about loading any type of standard or custom dataset at https://huggingface.co/docs/datasets/loading_datasets.html. endregion\n",
        "\n",
        "    # region Label preprocessing If you've passed us a training set, we try to infer your labels from it \n",
        "    # # 영역 레이블 사전 처리 교육 세트를 전달받으신 경우, 해당 레이블에서 유추하려고 합니다.\n",
        "    if \"train\" in datasets:\n",
        "        # By default we assume that if your label column looks like a float then you're doing regression, and if not then you're doing classification. This is something you may want to change!\n",
        "        # 기본적으로 레이블 열이 플로트처럼 보이면 회귀 분석을 수행하는 것이고 그렇지 않으면 분류를 수행하는 것이라고 가정합니다. 이것은 여러분이 바꾸고 싶을지도 모릅니다!\n",
        "        is_regression = datasets[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
        "        if is_regression:\n",
        "            num_labels = 1\n",
        "        else:\n",
        "            # A useful fast method:\n",
        "            # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
        "            label_list = datasets[\"train\"].unique(\"label\")\n",
        "            label_list.sort()  # Let's sort it for determinism\n",
        "            num_labels = len(label_list)\n",
        "    # If you haven't passed a training set, we read label info from the saved model (this happens later)\n",
        "    else:\n",
        "        num_labels = None\n",
        "        label_list = None\n",
        "        is_regression = None\n",
        "    # endregion\n",
        "\n",
        "    # region Load model config and tokenizer\n",
        "    if checkpoint is not None:\n",
        "        config_path = training_args.output_dir\n",
        "    elif model_args.config_name:\n",
        "        config_path = model_args.config_name\n",
        "    else:\n",
        "        config_path = model_args.model_name_or_path\n",
        "    if num_labels is not None:\n",
        "        config = AutoConfig.from_pretrained(\n",
        "            config_path,\n",
        "            num_labels=num_labels,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            revision=model_args.model_revision,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    else:\n",
        "        config = AutoConfig.from_pretrained(\n",
        "            config_path,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            revision=model_args.model_revision,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "        revision=model_args.model_revision,\n",
        "        use_auth_token=True if model_args.use_auth_token else None,\n",
        "    )\n",
        "    # endregion\n",
        "\n",
        "    # region Dataset preprocessing Again, we try to have some nice defaults but don't hesitate to tweak to your use case.\n",
        "    # 지역 데이터 세트 사전 처리 다시 한번, 우리는 좋은 기본값을 가지려고 노력하지만 당신의 사용 사례를 수정하기를 주저하지 않는다.\n",
        "    column_names = {col for cols in datasets.column_names.values() for col in cols}\n",
        "    non_label_column_names = [name for name in column_names if name != \"label\"]\n",
        "    if \"sentence1\" in non_label_column_names and \"sentence2\" in non_label_column_names:\n",
        "        sentence1_key, sentence2_key = \"sentence1\", \"sentence2\"\n",
        "    elif \"sentence1\" in non_label_column_names:\n",
        "        sentence1_key, sentence2_key = \"sentence1\", None\n",
        "    else:\n",
        "        if len(non_label_column_names) >= 2:\n",
        "            sentence1_key, sentence2_key = non_label_column_names[:2]\n",
        "        else:\n",
        "            sentence1_key, sentence2_key = non_label_column_names[0], None\n",
        "\n",
        "    if data_args.max_seq_length > tokenizer.model_max_length:\n",
        "        logger.warning(\n",
        "            f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the\"\n",
        "            f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
        "        )\n",
        "    max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n",
        "\n",
        "    # Ensure that our labels match the model's, if it has some pre-specified\n",
        "    if \"train\" in datasets:\n",
        "        if not is_regression and config.label2id != PretrainedConfig(num_labels=num_labels).label2id:\n",
        "            label_name_to_id = config.label2id\n",
        "            if list(sorted(label_name_to_id.keys())) == list(sorted(label_list)):\n",
        "                label_to_id = label_name_to_id  # Use the model's labels\n",
        "            else:\n",
        "                logger.warning(\n",
        "                    \"Your model seems to have been trained with labels, but they don't match the dataset: \",\n",
        "                    f\"model labels: {list(sorted(label_name_to_id.keys()))}, dataset labels: {list(sorted(label_list))}.\"\n",
        "                    \"\\nIgnoring the model labels as a result.\",\n",
        "                )\n",
        "                label_to_id = {v: i for i, v in enumerate(label_list)}\n",
        "        elif not is_regression:\n",
        "            label_to_id = {v: i for i, v in enumerate(label_list)}\n",
        "        else:\n",
        "            label_to_id = None\n",
        "        # Now we've established our label2id, let's overwrite the model config with it.\n",
        "        config.label2id = label_to_id\n",
        "        if config.label2id is not None:\n",
        "            config.id2label = {id: label for label, id in label_to_id.items()}\n",
        "        else:\n",
        "            config.id2label = None\n",
        "    else:\n",
        "        label_to_id = config.label2id  # Just load the data from the model\n",
        "\n",
        "    if \"validation\" in datasets and config.label2id is not None:\n",
        "        validation_label_list = datasets[\"validation\"].unique(\"label\")\n",
        "        for val_label in validation_label_list:\n",
        "            assert val_label in label_to_id, f\"Label {val_label} is in the validation set but not the training set!\"\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        # Tokenize the texts\n",
        "        args = (\n",
        "            (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
        "        )\n",
        "        result = tokenizer(*args, max_length=max_seq_length, truncation=True)\n",
        "\n",
        "        # Map labels to IDs\n",
        "        if config.label2id is not None and \"label\" in examples:\n",
        "            result[\"label\"] = [(config.label2id[l] if l != -1 else -1) for l in examples[\"label\"]]\n",
        "        return result\n",
        "\n",
        "    datasets = datasets.map(preprocess_function, batched=True, load_from_cache_file=not data_args.overwrite_cache)\n",
        "    # endregion\n",
        "\n",
        "    with training_args.strategy.scope():\n",
        "        # region Load pretrained model Set seed before initializing model\n",
        "        # 지역 부하 사전 교육된 모형 초기화 전 시드 설정\n",
        "        set_seed(training_args.seed)\n",
        "        #\n",
        "        # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently download model & vocab.\n",
        "        # 분산 교육에서 .from_preculated 방법은 하나의 로컬 프로세스만 동시에 모델과 보캅을 다운로드할 수 있도록 보장한다.\n",
        "        if checkpoint is None:\n",
        "            model_path = model_args.model_name_or_path\n",
        "        else:\n",
        "            model_path = checkpoint\n",
        "        model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "            model_path,\n",
        "            config=config,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            revision=model_args.model_revision,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "        # endregion\n",
        "\n",
        "        # region Optimizer, loss and compilation\n",
        "        optimizer = tf.keras.optimizers.Adam(\n",
        "            learning_rate=training_args.learning_rate,\n",
        "            beta_1=training_args.adam_beta1,\n",
        "            beta_2=training_args.adam_beta2,\n",
        "            epsilon=training_args.adam_epsilon,\n",
        "            clipnorm=training_args.max_grad_norm,\n",
        "        )\n",
        "        if is_regression:\n",
        "            loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "            metrics = []\n",
        "        else:\n",
        "            loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "            metrics = [\"accuracy\"]\n",
        "        model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
        "        # endregion\n",
        "\n",
        "        # region Convert data to a tf.data.Dataset\n",
        "\n",
        "        tf_data = dict()\n",
        "        max_samples = {\n",
        "            \"train\": data_args.max_train_samples,\n",
        "            \"validation\": data_args.max_val_samples,\n",
        "            \"test\": data_args.max_test_samples,\n",
        "        }\n",
        "        for key in (\"train\", \"validation\", \"test\"):\n",
        "            if key not in datasets:\n",
        "                tf_data[key] = None\n",
        "                continue\n",
        "            if key in (\"train\", \"validation\"):\n",
        "                assert \"label\" in datasets[key].features, f\"Missing labels from {key} data!\"\n",
        "            if key == \"train\":\n",
        "                shuffle = True\n",
        "                batch_size = training_args.per_device_train_batch_size\n",
        "                drop_remainder = True  # Saves us worrying about scaling gradients for the last batch\n",
        "            else:\n",
        "                shuffle = False\n",
        "                batch_size = training_args.per_device_eval_batch_size\n",
        "                drop_remainder = False\n",
        "            samples_limit = max_samples[key]\n",
        "            dataset = datasets[key]\n",
        "            if samples_limit is not None:\n",
        "                dataset = dataset.select(range(samples_limit))\n",
        "            if isinstance(training_args.strategy, tf.distribute.TPUStrategy) or data_args.pad_to_max_length:\n",
        "                logger.info(\"Padding all batches to max length because argument was set or we're on TPU.\")\n",
        "                dataset_mode = \"constant_batch\"\n",
        "            else:\n",
        "                dataset_mode = \"variable_batch\"\n",
        "            data = convert_dataset_for_tensorflow(\n",
        "                dataset,\n",
        "                non_label_column_names,\n",
        "                batch_size=batch_size,\n",
        "                dataset_mode=dataset_mode,\n",
        "                drop_remainder=drop_remainder,\n",
        "                shuffle=shuffle,\n",
        "            )\n",
        "            tf_data[key] = data\n",
        "        # endregion\n",
        "\n",
        "        # region Training and validation\n",
        "        if tf_data[\"train\"] is not None:\n",
        "            callbacks = [SavePretrainedCallback(output_dir=training_args.output_dir)]\n",
        "            model.fit(\n",
        "                tf_data[\"train\"],\n",
        "                validation_data=tf_data[\"validation\"],\n",
        "                epochs=int(training_args.num_train_epochs),\n",
        "                callbacks=callbacks,\n",
        "            )\n",
        "        elif tf_data[\"validation\"] is not None:\n",
        "            # If there's a validation dataset but no training set, just evaluate the metrics\n",
        "            logger.info(\"Computing metrics on validation data...\")\n",
        "            if is_regression:\n",
        "                loss = model.evaluate(tf_data[\"validation\"])\n",
        "                logger.info(f\"Loss: {loss:.5f}\")\n",
        "            else:\n",
        "                loss, accuracy = model.evaluate(tf_data[\"validation\"])\n",
        "                logger.info(f\"Loss: {loss:.5f}, Accuracy: {accuracy * 100:.4f}%\")\n",
        "        # endregion\n",
        "\n",
        "        # region Prediction\n",
        "        if tf_data[\"test\"] is not None:\n",
        "            logger.info(\"Doing predictions on test dataset...\")\n",
        "            predictions = model.predict(tf_data[\"test\"])[\"logits\"]\n",
        "            predicted_class = np.squeeze(predictions) if is_regression else np.argmax(predictions, axis=1)\n",
        "            output_test_file = os.path.join(training_args.output_dir, \"test_results.txt\")\n",
        "            with open(output_test_file, \"w\") as writer:\n",
        "                writer.write(\"index\\tprediction\\n\")\n",
        "                for index, item in enumerate(predicted_class):\n",
        "                    if is_regression:\n",
        "                        writer.write(f\"{index}\\t{item:3.3f}\\n\")\n",
        "                    else:\n",
        "                        item = config.id2label[item]\n",
        "                        writer.write(f\"{index}\\t{item}\\n\")\n",
        "            logger.info(f\"Wrote predictions to {output_test_file}!\")\n",
        "        # endregion\n",
        "\n",
        "    # region Prediction losses\n",
        "    # This section is outside the scope() because it's very quick to compute, but behaves badly inside it\n",
        "    if \"test\" in datasets and \"label\" in datasets[\"test\"].features:\n",
        "        print(\"Computing prediction loss on test labels...\")\n",
        "        labels = datasets[\"test\"][\"label\"]\n",
        "        loss = float(loss_fn(labels, predictions).numpy())\n",
        "        print(f\"Test loss: {loss:.4f}\")\n",
        "    # endregion\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "-cOHzC94WZfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "flSRgi58d9IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8si4s-Z7ivAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HN_JdJhkivYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "C3DWKQCDivfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "18mjhakyivl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# coding=utf-8\n",
        "# Copyright 2020 The HuggingFace Team. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" Fine-tuning the library models for sequence classification.\"\"\"\n",
        "\n",
        "\n",
        "import logging\n",
        "import os\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, Optional\n",
        "\n",
        "import datasets\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    EvalPrediction,\n",
        "    HfArgumentParser,\n",
        "    PreTrainedTokenizer,\n",
        "    TFAutoModelForSequenceClassification,\n",
        "    TFTrainer,\n",
        "    TFTrainingArguments,\n",
        ")\n",
        "from transformers.utils import logging as hf_logging\n",
        "\n",
        "\n",
        "hf_logging.set_verbosity_info()\n",
        "hf_logging.enable_default_handler()\n",
        "hf_logging.enable_explicit_format()\n",
        "\n",
        "\n",
        "def get_tfds(\n",
        "    train_file: str,\n",
        "    eval_file: str,\n",
        "    test_file: str,\n",
        "    tokenizer: PreTrainedTokenizer,\n",
        "    label_column_id: int,\n",
        "    max_seq_length: Optional[int] = None,\n",
        "):\n",
        "    files = {}\n",
        "\n",
        "    if train_file is not None:\n",
        "        files[datasets.Split.TRAIN] = [train_file]\n",
        "    if eval_file is not None:\n",
        "        files[datasets.Split.VALIDATION] = [eval_file]\n",
        "    if test_file is not None:\n",
        "        files[datasets.Split.TEST] = [test_file]\n",
        "\n",
        "    ds = datasets.load_dataset(\"csv\", data_files=files)\n",
        "    features_name = list(ds[list(files.keys())[0]].features.keys())\n",
        "    label_name = features_name.pop(label_column_id)\n",
        "    label_list = list(set(ds[list(files.keys())[0]][label_name]))\n",
        "    label2id = {label: i for i, label in enumerate(label_list)}\n",
        "    input_names = tokenizer.model_input_names\n",
        "    transformed_ds = {}\n",
        "\n",
        "    if len(features_name) == 1:\n",
        "        for k in files.keys():\n",
        "            transformed_ds[k] = ds[k].map(\n",
        "                lambda example: tokenizer.batch_encode_plus(\n",
        "                    example[features_name[0]], truncation=True, max_length=max_seq_length, padding=\"max_length\"\n",
        "                ),\n",
        "                batched=True,\n",
        "            )\n",
        "    elif len(features_name) == 2:\n",
        "        for k in files.keys():\n",
        "            transformed_ds[k] = ds[k].map(\n",
        "                lambda example: tokenizer.batch_encode_plus(\n",
        "                    (example[features_name[0]], example[features_name[1]]),\n",
        "                    truncation=True,\n",
        "                    max_length=max_seq_length,\n",
        "                    padding=\"max_length\",\n",
        "                ),\n",
        "                batched=True,\n",
        "            )\n",
        "\n",
        "    def gen_train():\n",
        "        for ex in transformed_ds[datasets.Split.TRAIN]:\n",
        "            d = {k: v for k, v in ex.items() if k in input_names}\n",
        "            label = label2id[ex[label_name]]\n",
        "            yield (d, label)\n",
        "\n",
        "    def gen_val():\n",
        "        for ex in transformed_ds[datasets.Split.VALIDATION]:\n",
        "            d = {k: v for k, v in ex.items() if k in input_names}\n",
        "            label = label2id[ex[label_name]]\n",
        "            yield (d, label)\n",
        "\n",
        "    def gen_test():\n",
        "        for ex in transformed_ds[datasets.Split.TEST]:\n",
        "            d = {k: v for k, v in ex.items() if k in input_names}\n",
        "            label = label2id[ex[label_name]]\n",
        "            yield (d, label)\n",
        "\n",
        "    train_ds = (\n",
        "        tf.data.Dataset.from_generator(\n",
        "            gen_train,\n",
        "            ({k: tf.int32 for k in input_names}, tf.int64),\n",
        "            ({k: tf.TensorShape([None]) for k in input_names}, tf.TensorShape([])),\n",
        "        )\n",
        "        if datasets.Split.TRAIN in transformed_ds\n",
        "        else None\n",
        "    )\n",
        "\n",
        "    if train_ds is not None:\n",
        "        train_ds = train_ds.apply(tf.data.experimental.assert_cardinality(len(ds[datasets.Split.TRAIN])))\n",
        "\n",
        "    val_ds = (\n",
        "        tf.data.Dataset.from_generator(\n",
        "            gen_val,\n",
        "            ({k: tf.int32 for k in input_names}, tf.int64),\n",
        "            ({k: tf.TensorShape([None]) for k in input_names}, tf.TensorShape([])),\n",
        "        )\n",
        "        if datasets.Split.VALIDATION in transformed_ds\n",
        "        else None\n",
        "    )\n",
        "\n",
        "    if val_ds is not None:\n",
        "        val_ds = val_ds.apply(tf.data.experimental.assert_cardinality(len(ds[datasets.Split.VALIDATION])))\n",
        "\n",
        "    test_ds = (\n",
        "        tf.data.Dataset.from_generator(\n",
        "            gen_test,\n",
        "            ({k: tf.int32 for k in input_names}, tf.int64),\n",
        "            ({k: tf.TensorShape([None]) for k in input_names}, tf.TensorShape([])),\n",
        "        )\n",
        "        if datasets.Split.TEST in transformed_ds\n",
        "        else None\n",
        "    )\n",
        "\n",
        "    if test_ds is not None:\n",
        "        test_ds = test_ds.apply(tf.data.experimental.assert_cardinality(len(ds[datasets.Split.TEST])))\n",
        "\n",
        "    return train_ds, val_ds, test_ds, label2id\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    Using `HfArgumentParser` we can turn this class\n",
        "    into argparse arguments to be able to specify them on\n",
        "    the command line.\n",
        "    \"\"\"\n",
        "\n",
        "    label_column_id: int = field(metadata={\"help\": \"Which column contains the label\"})\n",
        "    train_file: str = field(default=None, metadata={\"help\": \"The path of the training file\"})\n",
        "    dev_file: Optional[str] = field(default=None, metadata={\"help\": \"The path of the development file\"})\n",
        "    test_file: Optional[str] = field(default=None, metadata={\"help\": \"The path of the test file\"})\n",
        "    max_seq_length: int = field(\n",
        "        default=128,\n",
        "        metadata={\n",
        "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "            \"than this will be truncated, sequences shorter will be padded.\"\n",
        "        },\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name_or_path: str = field(\n",
        "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
        "    )\n",
        "    config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
        "    )\n",
        "    tokenizer_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
        "    )\n",
        "    use_fast: bool = field(default=False, metadata={\"help\": \"Set this flag to use fast tokenization.\"})\n",
        "    # If you want to tweak more attributes on your tokenizer, you should do it in a distinct script,\n",
        "    # or just modify its tokenizer_config.json.\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
        "    )\n",
        "\n",
        "\n",
        "def main():\n",
        "    # See all possible arguments in src/transformers/training_args.py\n",
        "    # or by passing the --help flag to this script.\n",
        "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
        "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TFTrainingArguments))\n",
        "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
        "\n",
        "    if (\n",
        "        os.path.exists(training_args.output_dir)\n",
        "        and os.listdir(training_args.output_dir)\n",
        "        and training_args.do_train\n",
        "        and not training_args.overwrite_output_dir\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
        "        )\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "    logger.info(\n",
        "        f\"n_replicas: {training_args.n_replicas}, distributed training: {bool(training_args.n_replicas > 1)}, \"\n",
        "        f\"16-bits training: {training_args.fp16}\"\n",
        "    )\n",
        "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    #\n",
        "    # Distributed training:\n",
        "    # The .from_pretrained methods guarantee that only one local process can concurrently\n",
        "    # download model & vocab.\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "    )\n",
        "\n",
        "    train_dataset, eval_dataset, test_ds, label2id = get_tfds(\n",
        "        train_file=data_args.train_file,\n",
        "        eval_file=data_args.dev_file,\n",
        "        test_file=data_args.test_file,\n",
        "        tokenizer=tokenizer,\n",
        "        label_column_id=data_args.label_column_id,\n",
        "        max_seq_length=data_args.max_seq_length,\n",
        "    )\n",
        "\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
        "        num_labels=len(label2id),\n",
        "        label2id=label2id,\n",
        "        id2label={id: label for label, id in label2id.items()},\n",
        "        finetuning_task=\"text-classification\",\n",
        "        cache_dir=model_args.cache_dir,\n",
        "    )\n",
        "\n",
        "    with training_args.strategy.scope():\n",
        "        model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            from_pt=bool(\".bin\" in model_args.model_name_or_path),\n",
        "            config=config,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "        )\n",
        "\n",
        "    def compute_metrics(p: EvalPrediction) -> Dict:\n",
        "        preds = np.argmax(p.predictions, axis=1)\n",
        "\n",
        "        return {\"acc\": (preds == p.label_ids).mean()}\n",
        "\n",
        "    # Initialize our Trainer\n",
        "    trainer = TFTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    if training_args.do_train:\n",
        "        trainer.train()\n",
        "        trainer.save_model()\n",
        "        tokenizer.save_pretrained(training_args.output_dir)\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if training_args.do_eval:\n",
        "        logger.info(\"*** Evaluate ***\")\n",
        "        result = trainer.evaluate()\n",
        "        output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\n",
        "\n",
        "        with open(output_eval_file, \"w\") as writer:\n",
        "            logger.info(\"***** Eval results *****\")\n",
        "\n",
        "            for key, value in result.items():\n",
        "                logger.info(f\"  {key} = {value}\")\n",
        "                writer.write(f\"{key} = {value}\\n\")\n",
        "\n",
        "            results.update(result)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "H5gz_N0PivsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rJHfiRxyiwPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jU8Vs1_unvT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rwCfzb5znvZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oIDXGKivnvfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# coding=utf-8\n",
        "# Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" Finetuning the library models for sequence classification on GLUE.\"\"\"\n",
        "# You can also adapt this script on your own text classification task. Pointers for this are left as comments.\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    HfArgumentParser,\n",
        "    PretrainedConfig,\n",
        "    TFAutoModelForSequenceClassification,\n",
        "    TFTrainingArguments,\n",
        "    set_seed,\n",
        ")\n",
        "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
        "from transformers.utils import check_min_version\n",
        "\n",
        "\n",
        "# region Helper functions\n",
        "\n",
        "\n",
        "def convert_dataset_for_tensorflow(\n",
        "    dataset, non_label_column_names, batch_size, dataset_mode=\"variable_batch\", shuffle=True, drop_remainder=True\n",
        "):\n",
        "    \"\"\"Converts a Hugging Face dataset to a Tensorflow Dataset. The dataset_mode controls whether we pad all batches\n",
        "    to the maximum sequence length, or whether we only pad to the maximum length within that batch. The former\n",
        "    is most useful when training on TPU, as a new graph compilation is required for each sequence length.\n",
        "    \"\"\"\n",
        "\n",
        "    def densify_ragged_batch(features, label=None):\n",
        "        features = {\n",
        "            feature: ragged_tensor.to_tensor(shape=batch_shape[feature]) for feature, ragged_tensor in features.items()\n",
        "        }\n",
        "        if label is None:\n",
        "            return features\n",
        "        else:\n",
        "            return features, label\n",
        "\n",
        "    feature_keys = list(set(dataset.features.keys()) - set(non_label_column_names + [\"label\"]))\n",
        "    if dataset_mode == \"variable_batch\":\n",
        "        batch_shape = {key: None for key in feature_keys}\n",
        "        data = {key: tf.ragged.constant(dataset[key]) for key in feature_keys}\n",
        "    elif dataset_mode == \"constant_batch\":\n",
        "        data = {key: tf.ragged.constant(dataset[key]) for key in feature_keys}\n",
        "        batch_shape = {\n",
        "            key: tf.concat(([batch_size], ragged_tensor.bounding_shape()[1:]), axis=0)\n",
        "            for key, ragged_tensor in data.items()\n",
        "        }\n",
        "    else:\n",
        "        raise ValueError(\"Unknown dataset mode!\")\n",
        "\n",
        "    if \"label\" in dataset.features:\n",
        "        labels = tf.convert_to_tensor(np.array(dataset[\"label\"]))\n",
        "        tf_dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
        "    else:\n",
        "        tf_dataset = tf.data.Dataset.from_tensor_slices(data)\n",
        "    if shuffle:\n",
        "        tf_dataset = tf_dataset.shuffle(buffer_size=len(dataset))\n",
        "    tf_dataset = tf_dataset.batch(batch_size=batch_size, drop_remainder=drop_remainder).map(densify_ragged_batch)\n",
        "    return tf_dataset\n",
        "\n",
        "\n",
        "class SavePretrainedCallback(tf.keras.callbacks.Callback):\n",
        "    # Hugging Face models have a save_pretrained() method that saves both the weights and the necessary\n",
        "    # metadata to allow them to be loaded as a pretrained model in future. This is a simple Keras callback\n",
        "    # that saves the model with this method after each epoch.\n",
        "    def __init__(self, output_dir, **kwargs):\n",
        "        super().__init__()\n",
        "        self.output_dir = output_dir\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.model.save_pretrained(self.output_dir)\n",
        "\n",
        "\n",
        "# endregion\n",
        "\n",
        "\n",
        "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
        "#check_min_version(\"4.16.0.dev0\")\n",
        "\n",
        "task_to_keys = {\n",
        "    \"cola\": (\"sentence\", None),\n",
        "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
        "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
        "    \"qnli\": (\"question\", \"sentence\"),\n",
        "    \"qqp\": (\"question1\", \"question2\"),\n",
        "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
        "    \"sst2\": (\"sentence\", None),\n",
        "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
        "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
        "}\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "# region Command-line arguments\n",
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    Using `HfArgumentParser` we can turn this class\n",
        "    into argparse arguments to be able to specify them on\n",
        "    the command line.\n",
        "    \"\"\"\n",
        "\n",
        "    task_name: str = field(\n",
        "        metadata={\"help\": \"The name of the task to train on: \" + \", \".join(task_to_keys.keys())},\n",
        "    )\n",
        "    predict_file: str = field(\n",
        "        metadata={\"help\": \"A file containing user-supplied examples to make predictions for\"},\n",
        "        default=None,\n",
        "    )\n",
        "    max_seq_length: int = field(\n",
        "        default=128,\n",
        "        metadata={\n",
        "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "            \"than this will be truncated, sequences shorter will be padded.\"\n",
        "        },\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n",
        "    )\n",
        "    pad_to_max_length: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n",
        "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n",
        "        },\n",
        "    )\n",
        "    max_train_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
        "            \"value if set.\"\n",
        "        },\n",
        "    )\n",
        "    max_eval_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
        "            \"value if set.\"\n",
        "        },\n",
        "    )\n",
        "    max_predict_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
        "            \"value if set.\"\n",
        "        },\n",
        "    )\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.task_name = self.task_name.lower()\n",
        "        if self.task_name not in task_to_keys.keys():\n",
        "            raise ValueError(\"Unknown task, you should pick one in \" + \",\".join(task_to_keys.keys()))\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name_or_path: str = field(\n",
        "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
        "    )\n",
        "    config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
        "    )\n",
        "    tokenizer_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
        "    )\n",
        "    use_fast_tokenizer: bool = field(\n",
        "        default=True,\n",
        "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
        "    )\n",
        "    model_revision: str = field(\n",
        "        default=\"main\",\n",
        "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
        "    )\n",
        "    use_auth_token: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n",
        "            \"with private models).\"\n",
        "        },\n",
        "    )\n",
        "\n",
        "\n",
        "# endregion\n",
        "\n",
        "\n",
        "def main():\n",
        "    # region Argument parsing\n",
        "    # See all possible arguments in src/transformers/training_args.py\n",
        "    # or by passing the --help flag to this script.\n",
        "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
        "\n",
        "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TFTrainingArguments))\n",
        "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
        "        # If we pass only one argument to the script and it's the path to a json file,\n",
        "        # let's parse it to get our arguments.\n",
        "        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
        "    else:\n",
        "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
        "\n",
        "    if not (training_args.do_train or training_args.do_eval or training_args.do_predict):\n",
        "        exit(\"Must specify at least one of --do_train, --do_eval or --do_predict!\")\n",
        "    # endregion\n",
        "\n",
        "    # region Checkpoints\n",
        "    checkpoint = None\n",
        "    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
        "        checkpoint = get_last_checkpoint(training_args.output_dir)\n",
        "        if checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
        "            raise ValueError(\n",
        "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
        "                \"Use --overwrite_output_dir to overcome.\"\n",
        "            )\n",
        "        elif checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
        "            logger.info(\n",
        "                f\"Checkpoint detected, resuming training at {checkpoint}. To avoid this behavior, change \"\n",
        "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
        "            )\n",
        "    # endregion\n",
        "\n",
        "    # region Logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        handlers=[logging.StreamHandler(sys.stdout)],\n",
        "    )\n",
        "    logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)\n",
        "\n",
        "    # Set the verbosity to info of the Transformers logger (on main process only):\n",
        "    if is_main_process(training_args.local_rank):\n",
        "        transformers.utils.logging.set_verbosity_info()\n",
        "        transformers.utils.logging.enable_default_handler()\n",
        "        transformers.utils.logging.enable_explicit_format()\n",
        "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
        "    # endregion\n",
        "\n",
        "    # region Dataset and labels\n",
        "    # Set seed before initializing model.\n",
        "    set_seed(training_args.seed)\n",
        "\n",
        "    # Downloading and loading a dataset from the hub. In distributed training, the load_dataset function guarantee\n",
        "    # that only one local process can concurrently download the dataset.\n",
        "    datasets = load_dataset(\"glue\", data_args.task_name, cache_dir=model_args.cache_dir)\n",
        "    # See more about loading any type of standard or custom dataset at\n",
        "    # https://huggingface.co/docs/datasets/loading_datasets.html.\n",
        "\n",
        "    is_regression = data_args.task_name == \"stsb\"\n",
        "    if not is_regression:\n",
        "        label_list = datasets[\"train\"].features[\"label\"].names\n",
        "        num_labels = len(label_list)\n",
        "    else:\n",
        "        num_labels = 1\n",
        "\n",
        "    if data_args.predict_file is not None:\n",
        "        logger.info(\"Preparing user-supplied file for predictions...\")\n",
        "\n",
        "        data_files = {\"data\": data_args.predict_file}\n",
        "\n",
        "        for key in data_files.keys():\n",
        "            logger.info(f\"Loading a local file for {key}: {data_files[key]}\")\n",
        "\n",
        "        if data_args.predict_file.endswith(\".csv\"):\n",
        "            # Loading a dataset from local csv files\n",
        "            user_dataset = load_dataset(\"csv\", data_files=data_files, cache_dir=model_args.cache_dir)\n",
        "        else:\n",
        "            # Loading a dataset from local json files\n",
        "            user_dataset = load_dataset(\"json\", data_files=data_files, cache_dir=model_args.cache_dir)\n",
        "        needed_keys = task_to_keys[data_args.task_name]\n",
        "        for key in needed_keys:\n",
        "            assert key in user_dataset[\"data\"].features, f\"Your supplied predict_file is missing the {key} key!\"\n",
        "        datasets[\"user_data\"] = user_dataset[\"data\"]\n",
        "    # endregion\n",
        "\n",
        "    # region Load model config and tokenizer\n",
        "    #\n",
        "    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
        "    # download model & vocab.\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
        "        num_labels=num_labels,\n",
        "        finetuning_task=data_args.task_name,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "        revision=model_args.model_revision,\n",
        "        use_auth_token=True if model_args.use_auth_token else None,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "        use_fast=model_args.use_fast_tokenizer,\n",
        "        revision=model_args.model_revision,\n",
        "        use_auth_token=True if model_args.use_auth_token else None,\n",
        "    )\n",
        "    # endregion\n",
        "\n",
        "    # region Dataset preprocessing\n",
        "    sentence1_key, sentence2_key = task_to_keys[data_args.task_name]\n",
        "    non_label_column_names = [name for name in datasets[\"train\"].column_names if name != \"label\"]\n",
        "\n",
        "    # Padding strategy\n",
        "    if data_args.pad_to_max_length:\n",
        "        padding = \"max_length\"\n",
        "    else:\n",
        "        # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n",
        "        padding = False\n",
        "\n",
        "    # Some models have set the order of the labels to use, so let's make sure we do use it.\n",
        "    label_to_id = None\n",
        "    if config.label2id != PretrainedConfig(num_labels=num_labels).label2id and not is_regression:\n",
        "        # Some have all caps in their config, some don't.\n",
        "        label_name_to_id = {k.lower(): v for k, v in config.label2id.items()}\n",
        "        if list(sorted(label_name_to_id.keys())) == list(sorted(label_list)):\n",
        "            label_to_id = {i: int(label_name_to_id[label_list[i]]) for i in range(num_labels)}\n",
        "        else:\n",
        "            logger.warning(\n",
        "                \"Your model seems to have been trained with labels, but they don't match the dataset: \",\n",
        "                f\"model labels: {list(sorted(label_name_to_id.keys()))}, dataset labels: {list(sorted(label_list))}.\"\n",
        "                \"\\nIgnoring the model labels as a result.\",\n",
        "            )\n",
        "            label_to_id = {label: i for i, label in enumerate(label_list)}\n",
        "    if label_to_id is not None:\n",
        "        config.label2id = label_to_id\n",
        "        config.id2label = {id: label for label, id in config.label2id.items()}\n",
        "    elif data_args.task_name is not None and not is_regression:\n",
        "        config.label2id = {l: i for i, l in enumerate(label_list)}\n",
        "        config.id2label = {id: label for label, id in config.label2id.items()}\n",
        "\n",
        "    if data_args.max_seq_length > tokenizer.model_max_length:\n",
        "        logger.warning(\n",
        "            f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the\"\n",
        "            f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
        "        )\n",
        "    max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        # Tokenize the texts\n",
        "        args = (\n",
        "            (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
        "        )\n",
        "        result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)\n",
        "\n",
        "        return result\n",
        "\n",
        "    datasets = datasets.map(preprocess_function, batched=True, load_from_cache_file=not data_args.overwrite_cache)\n",
        "\n",
        "    # endregion\n",
        "\n",
        "    # region Metric function\n",
        "    metric = load_metric(\"glue\", data_args.task_name)\n",
        "\n",
        "    def compute_metrics(preds, label_ids):\n",
        "        preds = preds[\"logits\"]\n",
        "        preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n",
        "        result = metric.compute(predictions=preds, references=label_ids)\n",
        "        if len(result) > 1:\n",
        "            result[\"combined_score\"] = np.mean(list(result.values())).item()\n",
        "        return result\n",
        "\n",
        "    # endregion\n",
        "\n",
        "    with training_args.strategy.scope():\n",
        "        # region Load pretrained model\n",
        "        if checkpoint is None:\n",
        "            model_path = model_args.model_name_or_path\n",
        "        else:\n",
        "            model_path = checkpoint\n",
        "        model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "            model_path,\n",
        "            config=config,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            revision=model_args.model_revision,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "        )\n",
        "        # endregion\n",
        "\n",
        "        # region Optimizer, loss and compilation\n",
        "        optimizer = tf.keras.optimizers.Adam(\n",
        "            learning_rate=training_args.learning_rate,\n",
        "            beta_1=training_args.adam_beta1,\n",
        "            beta_2=training_args.adam_beta2,\n",
        "            epsilon=training_args.adam_epsilon,\n",
        "            clipnorm=training_args.max_grad_norm,\n",
        "        )\n",
        "        if is_regression:\n",
        "            loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "            metrics = []\n",
        "        else:\n",
        "            loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "            metrics = [\"accuracy\"]\n",
        "        model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
        "        # endregion\n",
        "\n",
        "        # region Convert data to a tf.data.Dataset\n",
        "        tf_data = dict()\n",
        "        if isinstance(training_args.strategy, tf.distribute.TPUStrategy) or data_args.pad_to_max_length:\n",
        "            logger.info(\"Padding all batches to max length because argument was set or we're on TPU.\")\n",
        "            dataset_mode = \"constant_batch\"\n",
        "        else:\n",
        "            dataset_mode = \"variable_batch\"\n",
        "        max_samples = {\n",
        "            \"train\": data_args.max_train_samples,\n",
        "            \"validation\": data_args.max_eval_samples,\n",
        "            \"validation_matched\": data_args.max_eval_samples,\n",
        "            \"validation_mismatched\": data_args.max_eval_samples,\n",
        "            \"test\": data_args.max_predict_samples,\n",
        "            \"test_matched\": data_args.max_predict_samples,\n",
        "            \"test_mismatched\": data_args.max_predict_samples,\n",
        "            \"user_data\": None,\n",
        "        }\n",
        "        for key in datasets.keys():\n",
        "            if key == \"train\" or key.startswith(\"validation\"):\n",
        "                assert \"label\" in datasets[key].features, f\"Missing labels from {key} data!\"\n",
        "            if key == \"train\":\n",
        "                shuffle = True\n",
        "                batch_size = training_args.per_device_train_batch_size\n",
        "                drop_remainder = True  # Saves us worrying about scaling gradients for the last batch\n",
        "            else:\n",
        "                shuffle = False\n",
        "                batch_size = training_args.per_device_eval_batch_size\n",
        "                drop_remainder = False\n",
        "            samples_limit = max_samples[key]\n",
        "            dataset = datasets[key]\n",
        "            if samples_limit is not None:\n",
        "                dataset = dataset.select(range(samples_limit))\n",
        "            data = convert_dataset_for_tensorflow(\n",
        "                dataset,\n",
        "                non_label_column_names,\n",
        "                batch_size=batch_size,\n",
        "                dataset_mode=dataset_mode,\n",
        "                drop_remainder=drop_remainder,\n",
        "                shuffle=shuffle,\n",
        "            )\n",
        "            tf_data[key] = data\n",
        "        # endregion\n",
        "\n",
        "        # region Training and validation\n",
        "        if training_args.do_train:\n",
        "            callbacks = [SavePretrainedCallback(output_dir=training_args.output_dir)]\n",
        "            if training_args.do_eval and not data_args.task_name == \"mnli\":\n",
        "                # Do both evaluation and training in the Keras fit loop, unless the task is MNLI\n",
        "                # because MNLI has two validation sets\n",
        "                validation_data = tf_data[\"validation\"]\n",
        "            else:\n",
        "                validation_data = None\n",
        "            model.fit(\n",
        "                tf_data[\"train\"],\n",
        "                validation_data=validation_data,\n",
        "                epochs=int(training_args.num_train_epochs),\n",
        "                callbacks=callbacks,\n",
        "            )\n",
        "        # endregion\n",
        "\n",
        "        # region Evaluation\n",
        "        if training_args.do_eval:\n",
        "            # We normally do validation as part of the Keras fit loop, but we run it independently\n",
        "            # if there was no fit() step (because we didn't train the model) or if the task is MNLI,\n",
        "            # because MNLI has a separate validation-mismatched validation set\n",
        "            logger.info(\"*** Evaluate ***\")\n",
        "\n",
        "            # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "            if data_args.task_name == \"mnli\":\n",
        "                tasks = [\"mnli\", \"mnli-mm\"]\n",
        "                tf_datasets = [tf_data[\"validation_matched\"], tf_data[\"validation_mismatched\"]]\n",
        "                raw_datasets = [datasets[\"validation_matched\"], datasets[\"validation_mismatched\"]]\n",
        "            else:\n",
        "                tasks = [data_args.task_name]\n",
        "                tf_datasets = [tf_data[\"validation\"]]\n",
        "                raw_datasets = [datasets[\"validation\"]]\n",
        "\n",
        "            for raw_dataset, tf_dataset, task in zip(raw_datasets, tf_datasets, tasks):\n",
        "                eval_predictions = model.predict(tf_dataset)\n",
        "                eval_metrics = compute_metrics(eval_predictions, raw_dataset[\"label\"])\n",
        "                print(f\"Evaluation metrics ({task}):\")\n",
        "                print(eval_metrics)\n",
        "\n",
        "        # endregion\n",
        "\n",
        "        # region Prediction\n",
        "        if training_args.do_predict or data_args.predict_file:\n",
        "            logger.info(\"*** Predict ***\")\n",
        "\n",
        "            # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "            tasks = []\n",
        "            tf_datasets = []\n",
        "            raw_datasets = []\n",
        "            if training_args.do_predict:\n",
        "                if data_args.task_name == \"mnli\":\n",
        "                    tasks.extend([\"mnli\", \"mnli-mm\"])\n",
        "                    tf_datasets.extend([tf_data[\"test_matched\"], tf_data[\"test_mismatched\"]])\n",
        "                    raw_datasets.extend([datasets[\"test_matched\"], datasets[\"test_mismatched\"]])\n",
        "                else:\n",
        "                    tasks.append(data_args.task_name)\n",
        "                    tf_datasets.append(tf_data[\"test\"])\n",
        "                    raw_datasets.append(datasets[\"test\"])\n",
        "            if data_args.predict_file:\n",
        "                tasks.append(\"user_data\")\n",
        "                tf_datasets.append(tf_data[\"user_data\"])\n",
        "                raw_datasets.append(datasets[\"user_data\"])\n",
        "\n",
        "            for raw_dataset, tf_dataset, task in zip(raw_datasets, tf_datasets, tasks):\n",
        "                test_predictions = model.predict(tf_dataset)\n",
        "                if \"label\" in raw_dataset:\n",
        "                    test_metrics = compute_metrics(test_predictions, raw_dataset[\"label\"])\n",
        "                    print(f\"Test metrics ({task}):\")\n",
        "                    print(test_metrics)\n",
        "\n",
        "                if is_regression:\n",
        "                    predictions_to_write = np.squeeze(test_predictions[\"logits\"])\n",
        "                else:\n",
        "                    predictions_to_write = np.argmax(test_predictions[\"logits\"], axis=1)\n",
        "\n",
        "                output_predict_file = os.path.join(training_args.output_dir, f\"predict_results_{task}.txt\")\n",
        "                with open(output_predict_file, \"w\") as writer:\n",
        "                    logger.info(f\"***** Writing prediction results for {task} *****\")\n",
        "                    writer.write(\"index\\tprediction\\n\")\n",
        "                    for index, item in enumerate(predictions_to_write):\n",
        "                        if is_regression:\n",
        "                            writer.write(f\"{index}\\t{item:3.3f}\\n\")\n",
        "                        else:\n",
        "                            item = model.config.id2label[item]\n",
        "                            writer.write(f\"{index}\\t{item}\\n\")\n",
        "        # endregion\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "BAzDT1LUnv6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "bTBQxwdLnwil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "YUuIH94GoZ_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\""
      ],
      "metadata": {
        "id": "F-r7k2_5pxfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "#!pip install datasets==1.6.1\n",
        "#!pip install scikit-learn==1.0"
      ],
      "metadata": {
        "id": "Fl14rF-qp9U8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bQNBjUU0r7ac"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}