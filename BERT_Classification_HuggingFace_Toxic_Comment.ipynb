{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT Classification - HuggingFace Toxic Comment.ipynb",
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJI5yFPU2O8TYiOSg8SwGA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinsusong/study-NLP-BERT/blob/main/BERT_Classification_HuggingFace_Toxic_Comment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT Classification - HuggingFace Toxic Comment"
      ],
      "metadata": {
        "id": "bp7pxaYeTV2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "딥러닝 용어 \n",
        "\n",
        "- 활성화 함수 : 신경망의 뉴런들을 선형 결합한 하면, 선형 모형이 된다. 이를 비선형 모형으로 결합하기 위해서는 활성화 함수가 필요\n",
        "\n",
        "- 옵티마이저 : 딥러닝은 뉴런에 적용되는 최적의 가중치 값을 찾기 위해 사용,  최적의 가중치 값은 손실함수(loss function)를 최소화하는 값\n",
        "\n",
        "- 손실함수 :  손실함수는 보통 에측값과 실제값의 차이로 정의, 손실함수의 값을 최소화하는 방향으로, 딥러닝의 가중치를 학습시킴, \n",
        "\n",
        "- 배치크기 : 배치크기는 딥러닝에 학습할 때, 입력하는 데이터의 수\n",
        "\n",
        "- 에포크 : 에포크는 학습 횟수이다. 에포크가 1이면 전체 데이터를 한 번 학습한다\n",
        "\n",
        "- 학습률 : 학습률은 기울기의 반대방향으로 이동할 때, 그 학습비율을 말함\n",
        "\n",
        "- \n",
        "\n"
      ],
      "metadata": {
        "id": "D_eQ9WmvZnT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이진 분류의 성능 평가 지표: \n",
        "1. Accuracy ( 정확도 ) : Accuracy는 실제 데이터에서 예측 데이터가 얼마나 같은지를 판단하는 단순한 지표\n",
        "\t- 한계 :   \n",
        "        1. 이진 분류의 경우, 데이터의 구성에 따라 모델의 성능을 왜곡할 수 있기 때문에 정확도 수치 하나만 가지고 성능을 평가하지 않는다.\n",
        "\t\t2. 정확도는 불균형한 레이블 값 분포에서 분류 모델의 성능을 판단할 경우, 적합한 평가 지표가 아니다.\n",
        "\n",
        "2. Confusion Matrix ( 오차 행렬 ) : 이진 분류에서 예측 오류가 얼마인지와 더불어 어떠한 유형의 예측 오류가 발생하고 있는지를 함께 나타내는 지표\n",
        "\t- True/False: 실제와 예측이 일치하는가?\n",
        "\t- Positive/Negative: 뭘로 예측했는가?\n",
        "\t\t1. TN(True Negative, Negative Negative): 실제는 Negative인데, Negative로 예측함.\n",
        "\t\t2. FP(False Positive, Negative Positive): 실제는 Negative인데, Positive로 예측함.\n",
        "\t\t3. FN(False Negative, Positive Negative): 실제는 Positive인데, Negative로 예측함.\n",
        "\t\t4. TP(True Positive, Positive Positive): 실제는 Positive인데, Positive로 예측함.\n",
        "\n",
        "\t- 불균형한 레이블 클래스를 가지는 데이터에서는\n",
        "\t\t1. 많은 데이터 중 중점적으로 찾아야 하는 매우 적은 수의 레이블에 1, \n",
        "        2. 그렇지 않은 경우에 0을 부여하는 경우가 많다.\n",
        "\n",
        "3. Precision ( 정밀도 ) , Recall ( 재현율 ) \n",
        "\t- Precision : FP(실제는 0인데 예측은 1)을 낮추는 데에 초점\n",
        "\t- Recall : FN(실제는 1인데 예측 0)을 낮추는 데에 초점\n",
        "\n",
        "\t- Task에 따른 Precision, Recall의 상대적 중요도 : 보통은 Recall이 Positive보다 상대적으로 중요한 업무가 많지만, Precision이 더 중요한 지표인 경우도 있다.\n",
        "\t\t\n",
        "\t\t1. Recall이 중요한 케이스 : 암 검출, 금융사기 검출 같은 Task에서는 실제로 Positive인 얘들을 Negative로 예측하면 큰일난다.\n",
        "\t\t2. Precision이 중요한 케이스 : 스팸 검출 같은 Task에서는 실제로 Negative인 얘들을 Positive로 예측하면 큰일난다.\n",
        "\n",
        "\t- Precision/Recall Trade-off : recision과 Recall은 상호 보완적인 평가 지표이기 때문에 어느 한 쪽을 강제로 높이면 다른 하나의 수치는 떨어지기 쉽다. 이를 Precision/Recall Trade-off라고 부른다.\n",
        "\n",
        "\n",
        "\n",
        "4. F1 score : F1 Score는 Precision과 Recall을 결합한 지표, Precision과 Recall이 어느 한 쪽으로 치우치지 않을 때 상대적으로 높은 값을 가진다.\n",
        "\t\n",
        "\n",
        "5. ROC Curve, AUC Score : ROC Curve와 이에 기반한 AUC Score는 이진 분류의 예측 성능 측정에서 중요하게 사용되는 지표\n",
        "\t\n",
        "\t- ROC Curve : ROC Curve가 가운데 직선에 가까울수록 성능이 떨어지는 것이며, 멀어질수록 성능이 뛰어난 것 \n",
        "\t\t1. FPR(False Positive Rate)이 변할 때 TPR(True Positive Rate)이 어떻게 변하는지를 나타내는 곡선\n",
        "\t\t2. FPR을 X축으로, TPR을 Y축으로 잡은 그래프\n",
        "\t\t\t- TPR(민감도) : 실제 값 Positive가 정확히 예측되어야 하는 수준 , Recall과 정의가 같\n",
        "\t\t\t- FPR : 1-TNR (특이성) : 실제 값 Negative가 정확히 예측되어야 하는 수준\n",
        "\n",
        "\t- ROC-AUC Score : 결국 분류의 성능 지표로 사용되는 것은 ROC 곡선 면적에 기반한 AUC(Area Under Curve) 값\n",
        "\t\t1. AUC(Area Under Curve) 값은 ROC 곡선 밑의 면적을 구한 것으로, 1에 가까울수록 좋은 수치\n",
        "\t\t2. AUC 값이 커지려면, FPR이 작은 상태에서 알마나 큰 TPR을 얻을 수 있느냐가 관건\n",
        "\t\t\n",
        "\n",
        "\t\t\n",
        "\n",
        "\t\t\t\n",
        "\t\t\t\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XUSbRSHgfmIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "huggingface transformers lib 사용을 위해 설치 "
      ],
      "metadata": {
        "id": "AqOjMXzuT_3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==2.3.0"
      ],
      "metadata": {
        "id": "2qcJHWE1TiJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "분석 할 데이터 압축 해제"
      ],
      "metadata": {
        "id": "wLc0vAJLUWEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_directory = 'jigsaw-toxic-comment-classification-challenge'"
      ],
      "metadata": {
        "id": "bKvsv5AST8Qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "지정 경로 설정해서 압축 해제 "
      ],
      "metadata": {
        "id": "l3As6ZoNUgn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!mkdir jigsaw-toxic-comment-classification-challenge\n",
        "!unzip {dataset_directory} -d jigsaw-toxic-comment-classification-challenge"
      ],
      "metadata": {
        "id": "jmLFwAHGUjyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!unzip {dataset_directory}/train.csv.zip -d data/\n",
        "!unzip {dataset_directory}/test.csv.zip -d data/\n",
        "!unzip {dataset_directory}/test_labels.csv.zip -d data/\n",
        "!unzip {dataset_directory}/sample_submission.csv.zip -d data/"
      ],
      "metadata": {
        "id": "qbNM2fedVbAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm # 반복문에서 진행상황 확인 가능 \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "tQTKlRgJUduB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Pipeline"
      ],
      "metadata": {
        "id": "Cdh49OpHpzxY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터 셋 로드 \n",
        "- 데이터 전처리 (Tokenization, Truncation & Padding)\n",
        "- 데이터 파이프라인 생성 : 데이터를 생성해서 무사히 저장하기까지 일련의 과정"
      ],
      "metadata": {
        "id": "ernZ7ZONV0i6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " train_path = 'data/train.csv'\n",
        " test_path = 'data/test.csv'\n",
        " test_labels_path = 'data/test_labels.csv'\n",
        " subm_path = 'data/sample_submission.csv'\n"
      ],
      "metadata": {
        "id": "KW3j0uMVWgsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_cols = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
        "\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_test = pd.read_csv(test_path)\n",
        "df_test_labels = pd.read_csv(test_labels_path)\n",
        "df_test_labels = df_test_labels.set_index('id') # 데이터 셋의 index를 id로 변경\n",
        "df_train.head()\n",
        "\n",
        "#df_submission = pd.read_csv(subm_path, index_col='id')\n",
        "\n",
        "#df_submission"
      ],
      "metadata": {
        "id": "XBOOTwtQWe_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "bert_model_name = 'bert-base-uncased'\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)\n",
        "# do_lower_case : 토큰화할 때 입력을 소문자로 할지 여부\n",
        "MAX_LEN = 128\n",
        "\n",
        "def tokenize_sentences(sentences, tokenizer, max_seq_len = 128):\n",
        "    tokenized_sentences = []\n",
        "\n",
        "    for sentence in tqdm(sentences):\n",
        "        tokenized_sentence = tokenizer.encode(\n",
        "            sentence, # Sentence to encode\n",
        "            add_special_tokens  = True , # Add '[CLS]' and '[SEP]'\n",
        "            max_length = max_seq_len, # Truencate all sentences,\n",
        "        )\n",
        "        tokenized_sentences.append(tokenized_sentence)\n",
        "    \n",
        "    return tokenized_sentences\n",
        "\n",
        "def create_attention_masks(tokenized_and_padded_sentences):\n",
        "    attention_masks = []\n",
        "\n",
        "    for sentence in tokenized_and_padded_sentences:\n",
        "        att_mask = [int(token_id > 0) for token_id in sentence]\n",
        "        attention_masks.append(att_mask)\n",
        "        \n",
        "    return np.asarray(attention_masks)\n",
        "\n",
        "# 토크나이징 작업\n",
        "input_ids = tokenize_sentences(df_train['comment_text'], tokenizer, MAX_LEN)\n",
        "print(\"토크나이저 후 \" , input_ids)\n",
        "\n",
        "# 패딩 작업 \n",
        "# maxlen : 정수, 모든 시퀀스의 최대 길이\n",
        "# dtype : 출력 시퀀스의 자료형. \n",
        "# padding: 'pre' 혹은 'post': 각 시퀀스의 처음 혹은 끝을 패딩\n",
        "# truncating: 'pre' 혹은 'post': maxlen보다 큰 시퀀스의 처음 혹은 끝의 값들을 제거\n",
        "# value: 부동소수점 혹은 문자열, 패딩할 값.\n",
        "\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "print(\"패딩 후 : \", input_ids)\n",
        "\n",
        "# 패딩 된 데이터를 기반으로 어텐션 마스크 생성\n",
        "attention_masks = create_attention_masks(input_ids)\n",
        "print(\"어텐션 마스크 생성 :\", attention_masks)\n"
      ],
      "metadata": {
        "id": "iQ3ZR8GTYjQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "scikit-learn의 패키지 안에 train_test_split 모듈을 활용하여 train set(학습 데이터 셋)과 validation set(테스트 셋)을 분리"
      ],
      "metadata": {
        "id": "ECszK79SppVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sFTUcT1VhsGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# scikit-learn의 패키지 안에 train_test_split 모듈을 활용하여 train set(학습 데이터 셋)과 validation set(테스트 셋)을 분리\n",
        "# 기존 train / test로 구분 되어 있었던 데이터 셋을 \n",
        "# train에서 train / validation으로 일정 비율 쪼갠 다음에 학습 시에는 train 셋으로 학습 후 중간중간 validation 셋으로 학습한 모델 평가\n",
        "# 매 epoch 마다 validation의 오차율을 확인하면서 과적합을 방지해야 좋은 성능의 모델을 만들 수 있다.\n",
        "\n",
        "\n",
        "labels =  df_train[label_cols].values\n",
        "print(df_train[label_cols])\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=0, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=0, test_size=0.1)\n",
        "\n",
        "train_size = len(train_inputs)\n",
        "validation_size = len(validation_inputs)\n"
      ],
      "metadata": {
        "id": "g9l_sy0HeHva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_inputs.shape)\n",
        "print(train_masks.shape)\n",
        "train_labels.shape"
      ],
      "metadata": {
        "id": "pKdWMzn1oh0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32 \n",
        "NR_EPOCHS  = 1\n",
        "\n",
        "def create_dataset(data_tuple, epochs=1, batch_size=32, buffer_size=10000, train=True):\n",
        "\n",
        "    # 데이터 배열이 메모리에 존재한다면, Dataset을 만드는 가장 간단한 방법은 Dataset.from_tensor_slices()를 사용하여 tf.Tensor로 변환하는 것\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(data_tuple) # from_tensor_slices : 차원을 맞춰서 데이터를 변환 ex) .slices(([1,2,],[3,4,],[5,6])) -> ((1,3,5),(2,4,6))\n",
        "    print(dataset)\n",
        "    if train: \n",
        "        dataset = dataset.shuffle(buffer_size=buffer_size) # shuffle : 버퍼에서 요소를 무작위로 샘플링하여 선택한 요소를 새 요소로 바꾼다.\n",
        "    \n",
        "    dataset = dataset.repeat(epochs) # repeat : 데이터 세트를 반복하여 생성 ex)  [1, 2, 3].repeat(3) -> [1,2,3,1,2,3,1,2,3,1,2,3]\n",
        "    dataset = dataset.batch(batch_size) # batch : 데이터 세트의 요소를 일괄 처리로 결합 ex) (1,2,3,4,5,6,7,8,9).batch_size(3) -> (1,2,3),(4,5,6),(7,8,9)\n",
        "\n",
        "    if train:\n",
        "        dataset = dataset.prefetch(1) # prefetch : Dataset이 데이터세트에서 요소를 미리 가져 오는 것 , \n",
        "\n",
        "    return dataset \n",
        "\n",
        "\n",
        "train_dataset = create_dataset((train_inputs, train_masks, train_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)\n",
        "validation_dataset = create_dataset((validation_inputs, validation_masks, validation_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Smc43DuKpssr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT Model "
      ],
      "metadata": {
        "id": "JyMAkPtKvHLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Transformers library에서  pretrained BERT base-model을 로드 \n",
        "- BERT 출력(CLS 토큰에 해당)에서 첫 번째 hidden-state 를 취하여 6개의 뉴런과 시그모이드 활성화(Classifier)가 있는 Dense layer에 공급한다. \n",
        "- 이 계층의 출력은 6개의 클래스 각각에 대한 확률로 해석될 수 있다.\n"
      ],
      "metadata": {
        "id": "JbGu5sz-wj3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertModel\n",
        "from tensorflow.keras.layers import Dense, Flatten \n",
        "import tensorflow as tf\n",
        "class BertClassifier(tf.keras.Model):\n",
        "    def __init__(self, bert: TFBertModel, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.bert = bert \n",
        "        self.classifier = Dense(num_classes, activation='sigmoid')\n",
        "        # Dense : 조밀하게 연결된 일반 NN 레이어 , 출력 공간의 차원 , 사용할 활성화 함수\n",
        "        # 활성화 함수 : \n",
        "        # relu :  은닉 층으로 학습 : 은닉층으로 역전파를 통해 좋은 성능이 나오기 때문에 마지막 층이 아니고서야 거의 relu 를 이용\n",
        "        # sigmond :  이진 분류 문제\n",
        "        # softmax : 클래스 분류 문제\n",
        "        # 마지막 층에서는 'sigmond' 와 'softmax' 를 사용함으로서 sigmond 는 'yes or no' 와 같은 이진 분류, softmax 는 확률 값을 내뱉어내 다양한 것을 분류하기 위함\n",
        "\n",
        "    @tf.function \n",
        "    def call(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n",
        "        outputs = self.bert(input_ids,\n",
        "                            attention_mask = attention_mask,\n",
        "                            token_type_ids=token_type_ids,\n",
        "                            position_ids= position_ids,\n",
        "                            head_mask=head_mask)\n",
        "        print(outputs)\n",
        "        print('--'*100)\n",
        "        #print(outputs[1])\n",
        "        #print('--'*100)\n",
        "        # print(outputs[1])\n",
        "        # print('--'*10)\n",
        "        cls_output = outputs[1]\n",
        "        cls_output = self.classifier(cls_output)\n",
        "        # print(cls_output)\n",
        "        # print('--'*10)\n",
        "        return cls_output\n",
        "model = BertClassifier(TFBertModel.from_pretrained(bert_model_name), len(label_cols))\n",
        "# huggingface.co 에서 모델 을 다운로드합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "2ZFkNTS2xNWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(input_ids[:1])\n"
      ],
      "metadata": {
        "id": "W6XL_SW17XxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "id": "HKT0DKsx8viP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training Loop"
      ],
      "metadata": {
        "id": "3VLX8JR2yh6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BinaryCrossentropy를 손실 함수로 사용합니다(출력 6개의 각 출력 뉴런에 대해 계산됨...).이는 6개의 이진 분류 작업을 동시에 교육하는 것과 같습니다.)\n",
        "\n",
        "- Transformers 라이브러리에서 AdamW optimizer를 1-cycle-policy과 함께 사용\n",
        "\n",
        "- AUC 평가 지표"
      ],
      "metadata": {
        "id": "LSGGrV9myt3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "H7ea2mxMiuix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time \n",
        "from transformers import create_optimizer\n",
        "\n",
        "#  steps_per_epoch : 전체데이터(train_size) // 한번 학습에 사용되는 데이터 수 (BATCH_SIZE)\n",
        "\n",
        "steps_per_epoch = train_size // BATCH_SIZE\n",
        "validation_steps = validation_size // BATCH_SIZE\n",
        "\n",
        "# | Loss Function : 예측 값과 실제 값의 차이를 수치로 표현, 손실함수를 최소값으로 만들기 위해 가중치를 업데이트 함 \n",
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss') # metrics.Mean : 주어진 값의 (가중된) 평균을 계산, ex) [1, 3, 5, 7]이면 평균은 4. 가중치가 [1, 1, 0, 0]으로 지정된 경우 평균은 2\n",
        "validation_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "# tf.keras.metrics.Mean : 훈련 중에 메트릭이 평가될 때 에포크/단계 사이에 호출\n",
        "\n",
        "# | Optimizer (with 1-cycle-policy) : 최적의 학습률을 찾는것 \n",
        "warmup_steps = steps_per_epoch // 3 \n",
        "total_steps = steps_per_epoch * NR_EPOCHS - warmup_steps \n",
        "optimizer = create_optimizer(init_lr=2e-5, num_train_steps=total_steps, num_warmup_steps=warmup_steps)\n",
        "\n",
        "# | Metrics\n",
        "# ROC-AUC Score : 결국 분류의 성능 지표로 사용되는 것, AUC(Area Under Curve) 값은 ROC 곡선 밑의 면적을 구한 것으로, 1에 가까울수록 좋은 수치\n",
        "train_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))] # metrixs.AUC() :  리만 합계를 통해 근사 AUC (곡선 아래 영역)를 계산\n",
        "validation_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(model, token_ids, masks, labels):\n",
        "    labels = tf.dtypes.cast(labels, tf.float32)\n",
        "\n",
        "    # with ... as 구문을 사용하게 되면 파일을 열고(open) 해당 구문이 끝나면 파일이 자동으로 닫히게 됨(close). \n",
        "    # with open(파일 경로, 모드) as 파일 객체:\n",
        "    # GradientTape : (자동 미분을 위해 기록)  테이프는 역방향 패스(오차 역전파)의 그래디언트를 계산하기 위해 정방향 패스에 기록할 연산을 알아야 합니다.\n",
        "    # TensorFlow 는 중간 연산 과정(함수, 연산)을 테이프(tape)에 차곡차곡 기록해주는 Gradient tapes 를 제공\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(token_ids, attention_mask=masks)\n",
        "        loss = loss_object(labels, predictions)\n",
        "    # loss : 예측 값과 실제 값의 차이를 수치화함( 손실함수 )\n",
        "        \n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables), 1.0)\n",
        "    # model.trainable_variables : 현재 모델에서의 가중치 중에서 학습해야 할 가중치는 trainable_variables에 있다.\n",
        "    # tape.gradient : 모델의 훈련 가능한 변수에 대한 그래디언트를 계산\n",
        "    # apply_gradients : 변수에 그라디언트를 적용\n",
        "\n",
        "\n",
        "    # loss 평균 값 구함.\n",
        "    train_loss(loss)\n",
        "\n",
        "    # train_auc_metrics : AUC (곡선 아래 영역)를 계산\n",
        "    for i, auc in enumerate(train_auc_metrics):\n",
        "        auc.update_state(labels[:,i], predictions[:,i])\n",
        "\n",
        "    \n",
        "@tf.function\n",
        "def validation_step(model, token_ids, masks, labels):\n",
        "    labels = tf.dtypes.cast(labels, tf.float32)\n",
        "\n",
        "    predictions = model(token_ids, attention_mask=masks, training=False)\n",
        "    v_loss = loss_object(labels, predictions)\n",
        "\n",
        "    validation_loss(v_loss)\n",
        "    for i, auc in enumerate(validation_auc_metrics):\n",
        "        auc.update_state(labels[:,i], predictions[:,i])\n",
        "\n",
        "def train(model, train_dataset, val_dataset, train_steps_per_epoch, val_steps_per_epoch, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        print('=' * 50, f\"EPOCH{epoch}\",'='*50)\n",
        "        start = time.time()\n",
        "\n",
        "\n",
        "        for i, (token_ids, masks, labels) in enumerate(tqdm(train_dataset, total=train_steps_per_epoch)):\n",
        "            train_step(model, token_ids, masks, labels)\n",
        "            if i % 1000 == 0:\n",
        "                print(f'\\nTrain Step: {i}, Loss: {train_loss.result()}')\n",
        "                for i, label_name in enumerate(label_cols):\n",
        "                    print(f\"{label_name} roc_auc {train_auc_metrics[i].result()}\")\n",
        "                    train_auc_metrics[i].reset_states()\n",
        "        \n",
        "        for i, (token_ids, masks, labels) in enumerate(tqdm(val_dataset, total=val_steps_per_epoch)):\n",
        "            validation_step(model, token_ids, masks, labels)\n",
        "        \n",
        "        print(f'\\nEpoch {epoch+1}, Validation Loss: {validation_loss.result()}, Time: {time.time()-start}\\n')\n",
        "\n",
        "        for i, label_name in enumerate(label_cols):\n",
        "            print(f\"{label_name} roc_auc {validation_auc_metrics[i].result()}\")\n",
        "            validation_auc_metrics[i].reset_states()\n",
        "\n",
        "        print('\\n')\n",
        "\n",
        "train(model, train_dataset, validation_dataset, train_steps_per_epoch=steps_per_epoch, val_steps_per_epoch=validation_steps, epochs=NR_EPOCHS)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j4ctoSBhzIZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run predictions on test-set & save submission"
      ],
      "metadata": {
        "id": "evhPrQ8Bw73V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_ids = tokenize_sentences(df_test['comment_text'],tokenizer, MAX_LEN)\n",
        "test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\",padding=\"post\")\n",
        "test_attention_masks = create_attention_masks(test_input_ids)\n",
        "\n"
      ],
      "metadata": {
        "id": "EQ3AOEd00hQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_BATCH_SIZE = 32 \n",
        "test_steps = len(df_test) // TEST_BATCH_SIZE\n",
        "\n",
        "test_dataset = create_dataset((test_input_ids, test_attention_masks), batch_size=TEST_BATCH_SIZE, train=False, epochs=1)\n",
        "\n",
        "df_submission = pd.read_csv(subm_path, index_col='id')\n",
        "\n",
        "for i, (token_ids, masks) in enumerate(tqdm(test_dataset, total=test_steps)):\n",
        "    sample_ids = df_test.iloc[i*TEST_BATCH_SIZE:(i+1)*TEST_BATCH_SIZE]['id']\n",
        "    predictions = model(token_ids, attention_mask=masks).numpy()\n",
        "\n",
        "    df_submission.loc[sample_ids, label_cols] = predictions\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VYFH3Sws05_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_submission.to_csv('submission.csv')"
      ],
      "metadata": {
        "id": "Vfi1Y-3S1q32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv('submission.csv').head()\n",
        "\n"
      ],
      "metadata": {
        "id": "HO7nh8e33OGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rFjgWf_fXg23"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}