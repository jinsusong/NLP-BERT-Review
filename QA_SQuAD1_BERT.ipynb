{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QA_SQuAD1_BERT.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPZHF8MmYE4nd6fktpHhkz6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinsusong/study-NLP-BERT/blob/main/QA_SQuAD1_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9583phXX_1kV"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "JGFMPvzZBfDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/data/squad1/\n",
        "%pwd"
      ],
      "metadata": {
        "id": "y39GbCnVFZN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -al"
      ],
      "metadata": {
        "id": "7FoZZ3LJFgzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 로드"
      ],
      "metadata": {
        "id": "dcBxl7BRK1Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "B0M6uL52UWk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's import the library. We typically only need at most four methods:\n",
        "from datasets import list_datasets, list_metrics, load_dataset, load_metric\n",
        "\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "LOlWkP4cFh9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Currently available datasets and metrics\n",
        "#datasets = list_datasets()\n",
        "#metrics = list_metrics()\n",
        "\n",
        "# print(f\" Currently {len(datasets)} datasets are available on the hub:\")\n",
        "# pprint(datasets, compact=True)\n",
        "# print(f\" Currently {len(metrics)} metrics are available on the hub:\")\n",
        "# pprint(metrics, compact=True)"
      ],
      "metadata": {
        "id": "yvNOxKJcM2JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can access various attributes of the datasets before downloading them\n",
        "#squad_dataset = list_datasets(with_details=True)[datasets.index('squad')]\n",
        "\n",
        "#pprint(squad_dataset.__dict__)  # It's a simple python dataclass"
      ],
      "metadata": {
        "id": "AeHB2dE4M7CP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading and loading a dataset\n",
        "dataset = load_dataset('squad', split='validation[:10%]')"
      ],
      "metadata": {
        "id": "8DPASnO7xZRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(dataset.info.__dict__)"
      ],
      "metadata": {
        "id": "XvQK3yRPxnJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)\n",
        "print(dataset['answers'])"
      ],
      "metadata": {
        "id": "GxZx1XeExqXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Dataset len(dataset): {len(dataset)}\")\n",
        "print(\"\\nFirst item 'dataset[0]':\")\n",
        "pprint(dataset[0])"
      ],
      "metadata": {
        "id": "lcpTiHwZxtBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSlice of the two items 'dataset[10:12]':\")\n",
        "pprint(dataset[10:12])"
      ],
      "metadata": {
        "id": "ClDB5G5ExyCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can get a full column of the dataset by indexing with its name as a string:\n",
        "print(dataset['question'][:10])"
      ],
      "metadata": {
        "id": "OH-atkUMyCMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.map(lambda example: print(len(example['context']), end=','))"
      ],
      "metadata": {
        "id": "MaqmfTzMyFqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "-jesz3nP1Tjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast, logging as transformers_logging\n",
        "\n",
        "transformers_logging.set_verbosity_warning()\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
      ],
      "metadata": {
        "id": "h7_TJwTj1cGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "9GzebQ361wHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(dataset['context'][0])"
      ],
      "metadata": {
        "id": "7zvbxT0J2pXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's batch tokenize our dataset 'context'\n",
        "encoded_dataset = dataset.map(lambda example: tokenizer(example['context']), batched=True)\n",
        "\n",
        "print(\"encoded_dataset[0]\")\n",
        "pprint(encoded_dataset[0], compact=True)"
      ],
      "metadata": {
        "id": "meDHp1AS1lRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let show a more complex processing with the full preparation of the SQuAD dataset\n",
        "# for training a model from Transformers\n",
        "def convert_to_features(batch):\n",
        "    # Tokenize contexts and questions (as pairs of inputs)\n",
        "    input_pairs = list(zip())\n",
        "    encodings = tokenizer(batch['context'], batch['question'], truncation=True)\n",
        "\n",
        "    # Compute start and end tokens for labels\n",
        "    start_positions, end_positions = [], []\n",
        "    for i, answer in enumerate(batch['answers']):\n",
        "        first_char = answer['answer_start'][0]\n",
        "        last_char = first_char + len(answer['text'][0]) - 1\n",
        "        start_positions.append(encodings.char_to_token(i, first_char))\n",
        "        end_positions.append(encodings.char_to_token(i, last_char))\n",
        "\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "    return encodings\n",
        "\n",
        "encoded_dataset = dataset.map(convert_to_features, batched=True)"
      ],
      "metadata": {
        "id": "8QTFqGFe10Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now our dataset comprise the labels for the start and end position\n",
        "# as well as the offsets for converting back tokens\n",
        "# in span of the original string for evaluation\n",
        "print(\"column_names\", encoded_dataset.column_names)\n",
        "print(\"start_positions\", encoded_dataset[:5]['start_positions'])"
      ],
      "metadata": {
        "id": "_y_2v04S17Nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Y2PlBbXc261A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}